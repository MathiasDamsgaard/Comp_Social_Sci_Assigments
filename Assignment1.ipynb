{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github Repository\n",
    "https://github.com/MathiasDamsgaard/Comp_Social_Sci_Assigments.git\n",
    "\n",
    "### Contribution statement\n",
    "We all helped each other with the different parts of the assignment, as we all sat toghether when reviewing the exercises from the first four weeks. While Andreas finished up part 1, Mathias corrected the answers for part 2, and Anton added the code for part 3 and part 4, before we all looked thorugh each others work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:50:48.495975600Z",
     "start_time": "2024-02-23T12:50:46.103372400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "import time\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise: Web-scraping the list of participants to the International Conference in Computational Social Science**    \n",
    ">\n",
    "> You can find the programme of the 2023 edition of the conference at [this link](https://ic2s2-2023.org/program). As you can see the conference programme included many different contributions: keynote presentations, parallel talks, tutorials, posters. \n",
    "> 1. Inspect the HTML of the page and use web-scraping to get the names of all researchers that contributed to the conference in 2023. The goal is the following: (i) get as many names as possible including: keynote speakers, chairs, authors of parallel talks and authors of posters; (ii) ensure that the collected names are complete and accuarate as reported in the website (e.g. both first name and family name); (iii) ensure that no name is repeated multiple times with slightly different spelling. \n",
    "> 2. Some instructions for success: \n",
    ">    * First, inspect the page through your web browser to identify the elements of the page that you want to collect. Ensure you understand the hierarchical structure of the page, and where the elements you are interested in are located within this nested structure.   \n",
    ">    * Use the [BeautifulSoup Python package](https://pypi.org/project/beautifulsoup4/) to navigate through the hierarchy and extract the elements you need from the page. \n",
    ">    * You can use the [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) method to find elements that match specific filters. Check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) of the library for detailed explanations on how to set filters.  \n",
    ">    * Parse the strings to ensure that you retrieve \"clean\" author names (e.g. remove commas, or other unwanted charachters)\n",
    ">    * The overall idea is to adapt the procedure I have used [here](https://nbviewer.org/github/lalessan/comsocsci2023/blob/master/additional_notebooks/ScreenScraping.ipynb) for the specific page you are scraping. \n",
    "> 3. Create the set of unique researchers that joined the conference and *store it into a file*.\n",
    ">     * *Important:* If you notice any issue with the list of names you have collected (e.g. duplicate/incorrect names), come up with a strategy to clean your list as much as possible. \n",
    "> 4. *Optional:* For a more complete represenation of the field, include in your list: (i) the names of researchers from the programme committee of the conference, that can be found at [this link](https://ic2s2-2023.org/program_committee); (ii) the organizers of tutorials, that can be found at [this link](https://ic2s2-2023.org/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The link to the conference program\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK) \n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(r.content) \n",
    "\n",
    "# Find the table with the keynote speakers\n",
    "table1 = soup.find(\"table\",{\"class\":\"tutorials\"})\n",
    "\n",
    "# Find all 'a' tags within the table as all the names are within 'a' tags\n",
    "links = table1.find_all('a')\n",
    "\n",
    "# Extract the text from each 'a' tag (which should be the names)\n",
    "keynote_names = [link.text for link in links if \"Keynote\" in link.text]\n",
    "\n",
    "# Split each string on ' - ' and get the second part as this is the name of the speaker\n",
    "# (.strip().title()) remove any leading/trailing white space and\n",
    "# convert the first letter to uppercase and the rest to lowercase in case of any mispellings\n",
    "names = [name.split(' - ')[1].strip().title() for name in keynote_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique researcher found are 1488\n"
     ]
    }
   ],
   "source": [
    "# Find all the tables with the keynote speakers\n",
    "# As we noticed that the plenary talkers all where italic we use that to find them all at once\n",
    "table2 = soup.find_all(\"div\",{\"class\":\"col-9 col-12-medium\"})\n",
    "\n",
    "# Loop through the tables and extract elements with 'i' tags\n",
    "for tab in table2:\n",
    "    plenary_talks = tab.find_all('i')\n",
    "# Extract the text from each 'i' tag (which should be the names)\n",
    "    plenary_talks = [talkers.text for talkers in plenary_talks]\n",
    "\n",
    "    # Process each string in the list\n",
    "    for talk in plenary_talks:\n",
    "        # If the string starts with 'Chair:', remove that part and trim whitespace\n",
    "        if talk.startswith('Chair:'):\n",
    "            talk = talk.replace('Chair:', '').strip().title()\n",
    "\n",
    "        # If the string contains a comma, split it into multiple names\n",
    "        if ',' in talk:\n",
    "            names.extend([name.strip().title() for name in talk.split(',')])\n",
    "\n",
    "        # If the string does not contain a comma, it's a single name. Pass the name to the list of names\n",
    "        else:\n",
    "            names.append(talk)\n",
    "    \n",
    "\n",
    "# Convert the list to a set, which removes duplicates and then back to a sorted list\n",
    "unique_names = sorted(set(names))\n",
    "\n",
    "# Print the length of the list with unique names\n",
    "print(f\"Number of unique researcher found are {len(unique_names)}\")\n",
    "\n",
    "\n",
    "# Create a dataframe from the list and save it to a CSV file\n",
    "df = pd.DataFrame(unique_names)\n",
    "df.to_csv('data/names.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. How many unique researchers do you get?\n",
    "\n",
    "We got 1488 unique researchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices __(answer in max 150 words)__.\n",
    "\n",
    "We scraped the names by first identifying the table containing the keynote speakers. All keynote speakers were represented by “a” tags. We extracted the text containing the word ‘Keynote’ from these tags, resulting in the 10 keynote speakers. For plenary speakers, we noticed their names were in italics (represented by “i” tags). We used this fact and extracted all the “i” tags from the rest of the webpage and handled three cases: names starting with ‘Chair:’, comma-separated names, and single names. This could be done as only names were written in italic, and we were therefore fairly confident we got about all the names out. \n",
    "Some of the strategies we used to make sure the names were accurately retrieved were by using like functions .strip() and .title() to eliminate whitespaces and standardize capitalization to circumvent any misspellings that would lead to errors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise: Ready made data vs Custom made data** In this exercise, I want to make sure you have understood they key points of my lecture and the reading. \n",
    ">\n",
    "> 1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book __(answer in max 150 words)__.\n",
    "\n",
    "- Benefits of custom-made data is the influence you have on collected the data and the representativeness, sensitivity, and completeness. A con is how resource heavy of a task it is. Often people are aware they are participating in an experiment, potentially making them reactive and behave a certain way. In Centola's experiment they did the study without letting the subjects knowing there was a test and control group, hence making their behaviour more believable.\n",
    "- It is often opposite for ready-made data, where you have access to big data, but not all of it may be useful to you. It requires a lot of clean-up and consideration of the truthfulness of the data. For Nicolaides's study the data matched what they wanted to do, however they have no way of knowing the individuals that they are analyzing, and whether the data reflects what they want to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. How do you think these differences can influence the interpretation of the results in each study? __(answer in max 150 words)__\n",
    "\n",
    "When making the choice of using either custom-made or ready-made data, the researchers must consider the benefits and consequences of that choice, here some of them were explained previously. If they fail to properly interpret possible trends, the relevancy, representativeness, or a possible confounder for the behaviour, the analysis suddenly losses its reliability. Here custom-made data will always be more reliable, but its higher cost and smaller sample size can lead to a skewed distribution of participants. These differences makes it hard of whether to rely on big data or not as the ten common characteristics also highlights relevant problems, some of which have already been explained. To do the necessary groundwork is therefore important, and if the considerations are properly presented, the influence of it on the results in a study as Nicolaides’ will become smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise : Collecting Research Articles from IC2S2 Authors**\n",
    ">\n",
    ">In this exercise, we'll leverage the OpenAlex API to gather information on research articles authored by participants of the IC2S2 2023 conference, referred to as *IC2S2 authors*. **Before you start, please ensure you read through the entire exercise.**\n",
    ">\n",
    "> \n",
    "> **Steps:**\n",
    ">  \n",
    "> 1. **Retrieve Data:** Starting with the *authors* you identified in Week 2, Exercise 2, use the OpenAlex API [works endpoint](https://docs.openalex.org/api-entities/works) to fetch the research articles they have authored. For each article, retrieve the following details:\n",
    ">    - _id_: The unique OpenAlex ID for the work.\n",
    ">    - _publication_year_: The year the work was published.\n",
    ">    - _cited_by_count_: The number of times the work has been cited by other works.\n",
    ">    - _author_ids_: The OpenAlex IDs for the authors of the work.\n",
    ">    - _title_: The title of the work.\n",
    ">    - _abstract_inverted_index_: The abstract of the work, formatted as an inverted index.\n",
    "> \n",
    ">     **Important Note on Paging:** By default, the OpenAlex API limits responses to 25 works per request. For more efficient data retrieval, I suggest to adjust this limit to 200 works per request. Even with this adjustment, you will need to implement pagination to access all available works for a given query. This ensures you can systematically retrieve the complete set of works beyond the initial 200. Find guidance on implementing pagination [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging).\n",
    ">\n",
    "> 2. **Data Storage:** Organize the retrieved information into two Pandas DataFrames and save them to two files in a suitable format:\n",
    ">    - The *IC2S2 papers* dataset should include: *id, publication\\_year, cited\\_by\\_count, author\\_ids*.\n",
    ">    - The *IC2S2 abstracts* dataset should include: *id, title, abstract\\_inverted\\_index*.\n",
    ">  \n",
    ">\n",
    "> **Filters:**\n",
    "> To ensure the data we collect is relevant and manageable, apply the following filters:\n",
    "> \n",
    ">    - Only include *IC2S2 authors* with a total work count between 5 and 5,000.\n",
    ">    - Retrieve only works that have received more than 10 citations.\n",
    ">    - Limit to works authored by fewer than 10 individuals.\n",
    ">    - Include only works relevant to Computational Social Science (focusing on: Sociology OR Psychology OR Economics OR Political Science) AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science), as defined by their [Concepts](https://docs.openalex.org/api-entities/works/work-object#concepts). *Note*: here we only consider Concepts at *level=0* (the most coarse definition of concepts). \n",
    ">\n",
    "> **Efficiency Tips:**\n",
    "> Writing efficient code in this exercise is **crucial**. To speed up your process:\n",
    "> - **Apply filters directly in your request:** When possible, use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) of the *works* endpoint to apply the filters above directly in your API request, ensuring only relevant data is returned. Learn about combining multiple filters [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists).  \n",
    "> - **Bulk requests:** Instead of sending one request for each author, you can use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) to query works by multiple authors in a single request. *Note: My testing suggests that can only include up to 25 authors per request.*\n",
    "> - **Use multiprocessing:** Implement multiprocessing to handle multiple requests simultaneously. I highly recommmend [Joblib’s Parallel](https://joblib.readthedocs.io/en/stable/) function for that, and [tqdm](https://tqdm.github.io/) can help monitor progress of your jobs. Remember to stay within [the rate limit](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication) of 10 requests per second.\n",
    ">\n",
    ">\n",
    ">   \n",
    "> For reference, employing these strategies allowed me to fetch the data in about 30 seconds using 5 cores on my laptop. I obtained a dataset of approximately 25 MB (including both the *IC2S2 abstracts* and *IC2S2 papers* files).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:38:34.971224Z",
     "start_time": "2024-02-22T09:38:34.938616700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data from Week 2 Part 3 - we uploaded the data from the exercise in the data folder ourselves instead of adding the code\n",
    "authors = pd.read_csv('data/authors.csv')\n",
    "authors = authors[(authors[\"works_count\"] >= 5) & (authors[\"works_count\"] <= 5000)]\n",
    "authors = authors['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:38:36.321261800Z",
     "start_time": "2024-02-22T09:38:35.111780900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We get the concepts for the different fields \n",
    "concept_url = 'https://api.openalex.org/concepts'\n",
    "filter_conc1 = 'level:0,display_name:Sociology|Psychology|Economics|Political science'\n",
    "filter_conc2 = 'level:0,display_name:Mathematics|Physics|Computer science'\n",
    "\n",
    "select = ['id']\n",
    "\n",
    "params1 = {'filter': filter_conc1, 'select': select}\n",
    "params2 = {'filter': filter_conc2, 'select': select}\n",
    "\n",
    "response1 = requests.get(concept_url, params=params1)\n",
    "response2 = requests.get(concept_url, params=params2)\n",
    "\n",
    "result1 = response1.json().get('results')\n",
    "result2 = response2.json().get('results')\n",
    "\n",
    "# We get the ids of the concepts to create the filters for the works\n",
    "concepts1 = '|'.join([concept['id'] for concept in result1])\n",
    "concepts2 = '|'.join([concept['id'] for concept in result2])\n",
    "\n",
    "# Furthermore we add the filters for the works with the found concepts\n",
    "work_url = 'https://api.openalex.org/works'\n",
    "select = 'id,publication_year,cited_by_count,authorships,title,abstract_inverted_index'\n",
    "filters = 'cited_by_count:>10,authors_count:<10,concepts.id:' + concepts1 + ',concepts.id:' + concepts2\n",
    "\n",
    "limit = 200\n",
    "\n",
    "params = {'filter': filters, 'select': select, 'per-page': limit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:38:36.335654400Z",
     "start_time": "2024-02-22T09:38:36.319211900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We create a function to get the data from the API, and handle the pagination and errors\n",
    "def fetch_data(author, url, params):\n",
    "    try:\n",
    "        params['filter'] = filters + ',author.id:' + '|'.join(author)\n",
    "        cursor = '*'\n",
    "        papers = []\n",
    "        abstracts = []\n",
    "\n",
    "        while cursor:\n",
    "            params['cursor'] = cursor\n",
    "            response = requests.get(url, params=params)\n",
    "            \n",
    "            # We handle different errors that can occur\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit exceeded. Sleeping for 2 seconds.\")\n",
    "                time.sleep(5)\n",
    "                response = requests.get(url, params=params)  # Retry the request\n",
    "            elif response.status_code == 403:\n",
    "                print(\"Forbidden access. Please check your API credentials or contact the API provider.\")\n",
    "                return [], []\n",
    "            \n",
    "            elif response.status_code != 200:\n",
    "                print(f\"Error occurred while fetching data for author {author}: {response.json()}\")\n",
    "                return [], []\n",
    "\n",
    "            data = response.json().get('results')\n",
    "            meta = response.json().get('meta')\n",
    "\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            # Retrieve the relevant data into the lists\n",
    "            for paper in data:\n",
    "                papers.append([paper['id'], paper['publication_year'], paper['cited_by_count'],\n",
    "                               [authorship['author']['id'] for authorship in paper['authorships']]])\n",
    "                \n",
    "                abstracts.append([paper['id'], paper['title'], paper['abstract_inverted_index']])\n",
    "\n",
    "            cursor = meta['next_cursor']  \n",
    "\n",
    "        return papers, abstracts\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching data for author {author}: {e}\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:42:59.336144700Z",
     "start_time": "2024-02-22T09:42:59.329735800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:34<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "papers_data = []\n",
    "abstracts_data = []\n",
    "\n",
    "# We use a bulk size of 25 authors to get data more quickly.\n",
    "bulk_size = 25\n",
    "author_chunks = [authors[i:i + bulk_size] for i in range(0, len(authors), bulk_size)]\n",
    "\n",
    "# We found that we could use 4 threads to get the data without hitting the rate limit.\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    # We fetch the data from the API using the function.\n",
    "    results = list(tqdm(executor.map(fetch_data, author_chunks, [work_url]*len(author_chunks),\n",
    "                                     [params]*len(author_chunks)), total=len(author_chunks)))\n",
    "    for result in results:\n",
    "        papers_data.extend(result[0])\n",
    "        abstracts_data.extend(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T09:42:59.331796Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 7726\n",
      "Total number of unique authors: 12247\n"
     ]
    }
   ],
   "source": [
    "# Organize the data into dataframes\n",
    "papers_df = pd.DataFrame(papers_data, columns=[\"id\", \"publication_year\", \"cited_by_count\", \"coauthor_ids\"])\n",
    "abstracts_df = pd.DataFrame(abstracts_data, columns=[\"id\", \"title\", \"abstract_inverted_index\"])\n",
    "papers_df.drop_duplicates(inplace=True, subset=['id'])\n",
    "abstracts_df.drop_duplicates(inplace=True, subset=['id'])\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "papers_df.to_csv(\"data/IC2S2_papers.csv\", index=False)\n",
    "abstracts_df.to_csv(\"data/IC2S2_abstracts.csv\", index=False)\n",
    "\n",
    "# Print answers to the questions\n",
    "print(f\"Total number of papers: {len(papers_df)}\")\n",
    "print(f\"Total number of unique authors: {papers_df['coauthor_ids'].explode().nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Data Overview and Reflection questions:** Answer the following questions: \n",
    "> - **Dataset summary.** How many works are listed in your *IC2S2 papers* dataframe? How many unique researchers have co-authored these works? \n",
    "\n",
    "We get 7726 works in our IC2S2 papers dataframe, and 12247 unique researchers to have co-authored these works. However, the code seems to generate a different number each time, often varying with a coupld hundred, and the former numbers are the most recently generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Efficiency in code.** Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? __(answer in max 150 words)__\n",
    "\n",
    "We used the bulk size of 25 authors to get the data faster. We found that we could use 4 threads to get the data without hitting the rate limit of the api. This allowed us to get the data in a reasonable time of around 30 secounds. Furthermore, we used the filters directly in the request to get the data that we wanted. Additionally, we filtered the dataframe beforehand to ensure we only requested for authors who had enough works written. This allowed us to get the data that we wanted without having to filter it afterwards. This made the process more efficient. However we still had to handle the pagination and the errors that could occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Filtering Criteria and Dataset Relevance**. Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? __(answer in max 150 words)__\n",
    "\n",
    "Some of the filters makes sense to incorporate as looking for well cited works as this makes sure we are only considering works that have made an impact on the field. The same goes for removing authors with really few works, as they probably haven’t become a well-established author yet. This can of course affect mean they have made one good article we might not find tough. However, filtering for authors with less than 5001 works and works with less than 10 co-authors is more filters to reduce computation time and complexity and can influence the relevance of the works found. Lastly, the concept filters do limit the fields of the works quite a bit and could be expanded if one wanted to broaden the search more, but currently they do help in finding relevant works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise: Constructing the Computational Social Scientists Network**\n",
    ">\n",
    "> In this exercise, we will create a network of researchers in the field of Computational Social Science using the NetworkX library. In our network, nodes represent authors of academic papers, with a direct link from node _A_ to node _B_ indicating a joint paper written by both. The link's weight reflects the number of papers written by both _A_ and _B_.\n",
    ">\n",
    "> **Part 1: Network Construction**\n",
    ">\n",
    "> 1. **Weighted Edgelist Creation:** Start with your dataframe of *papers*. Construct a _weighted edgelist_ where each list element is a tuple containing three elements: the _author ids_ of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:51:20.806011500Z",
     "start_time": "2024-02-23T12:51:10.119302200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "papers = pd.read_csv('data/IC2S2_full_papers.csv')\n",
    "\n",
    "# Initialize a dictionary to store author pairs and their counts\n",
    "author_pairs = defaultdict(int)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for _, row in papers.iterrows():\n",
    "    # Convert the 'authorships' string into a list\n",
    "    authors_list = ast.literal_eval(row['coauthor_ids'])\n",
    "    \n",
    "    # Generate all combinations of authors for the current row\n",
    "    authors = combinations(authors_list, 2)\n",
    "\n",
    "    # For each combination, increment the count in the dictionary\n",
    "    for pair in authors:\n",
    "        author_pairs[pair] += 1\n",
    "\n",
    "# Convert the dictionary to a list of tuples (edgelist)\n",
    "edgelist = [(pair[0], pair[1], weight) for pair, weight in author_pairs.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. **Graph Construction:**\n",
    ">    - Use NetworkX to create an undirected [``Graph``](https://networkx.org/documentation/stable/reference/classes/graph.html).\n",
    ">    - Employ the [`add_weighted_edges_from`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_weighted_edges_from.html#networkx.Graph.add_weighted_edges_from) function to populate the graph with the weighted edgelist from step 1, creating a weighted, undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:51:22.796685300Z",
     "start_time": "2024-02-23T12:51:20.805010Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a weighted graph from the edgelist\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. **Node Attributes:**\n",
    ">    - For each node, add attributes for the author's _display name_, _country_, _citation count_, and the _year of their first publication_ in Computational Social Science. The _display name_ and _country_ can be retrieved from your _authors_ dataset. The _year of their first publication_ and the _citation count_  can be retrieved from the _papers_ dataset.\n",
    ">    - Save the network as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:56:48.184896400Z",
     "start_time": "2024-02-23T12:51:22.793693400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authors_data = pd.read_csv('data/IC2S2_full_authors.csv')\n",
    "\n",
    "papers['coauthor_ids'] = papers['coauthor_ids'].apply(ast.literal_eval)\n",
    "authors = np.hstack(papers['coauthor_ids'].values)\n",
    "authors = list(set(authors))\n",
    "\n",
    "# Only keep an author if they're in the authors_data\n",
    "authors = [author for author in authors if author in authors_data['id'].values]\n",
    "\n",
    "# Create a dictionary of author IDs and their corresponding total citation counts and first year of publication.\n",
    "additional_data = {}\n",
    "for author in authors:\n",
    "    # Get the citations by finding papers that each author has authored and summing the citations for those papers.\n",
    "    # Then get the first year of an authors publication by finding papers that each author has authored in and selecting the minimum value.\n",
    "    author_papers = papers[papers['coauthor_ids'].apply(lambda x: author in x)]\n",
    "    additional_data[author] = [author_papers['cited_by_count'].sum(), author_papers['publication_year'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:34.202285Z",
     "start_time": "2024-02-23T12:56:48.188886300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add node attributes\n",
    "for author in G.nodes:\n",
    "    if authors_data.loc[authors_data['id'] == author, 'display_name'].values.size == 0:\n",
    "        continue\n",
    "    G.nodes[author]['display_name'] = authors_data.loc[authors_data['id'] == author, 'display_name'].values[0]\n",
    "    G.nodes[author]['country_code'] = authors_data.loc[authors_data['id'] == author, 'country_code'].values[0]\n",
    "    G.nodes[author]['citations'] = additional_data[author][0]\n",
    "    G.nodes[author]['first_year'] = additional_data[author][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:40.287017400Z",
     "start_time": "2024-02-23T12:59:34.203281200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the network as a JSON file\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            if np.isnan(obj):\n",
    "                return None\n",
    "            elif np.isinf(obj):\n",
    "                return str(obj)\n",
    "        return super(MyEncoder, self).default(obj)\n",
    "\n",
    "# The above step was done to avoid the error of numpy.int64 not being serializable\n",
    "\n",
    "data = nx.node_link_data(G)\n",
    "with open('data/authors_network.json', 'w') as f:\n",
    "    json.dump(data, f, cls=MyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Part 2: Preliminary Network Analysis**\n",
    "> Now, with the network constructed, perform a basic analysis to explore its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:42.275732100Z",
     "start_time": "2024-02-23T12:59:40.292002900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the JSON file back into a networkx graph\n",
    "with open('data/authors_network.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "G = nx.node_link_graph(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. **Network Metrics:**\n",
    ">    - What is the total number of nodes (authors) and links (collaborations) in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:42.346509700Z",
     "start_time": "2024-02-23T12:59:42.278690800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes: 103424\n",
      "Total number of links: 364335\n"
     ]
    }
   ],
   "source": [
    "# Total number of nodes (authors) and links (collaborations)\n",
    "print(f'Total number of nodes: {G.number_of_nodes()}')\n",
    "print(f'Total number of links: {G.number_of_edges()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.\n",
    "\n",
    "As the network density is 0.00007, it means that the network is definitly sparse, as there are not nearly as many links as the maximum number of possible links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:42.432280900Z",
     "start_time": "2024-02-23T12:59:42.342521800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network density: 6.81227902439113e-05\n"
     ]
    }
   ],
   "source": [
    "# Network density\n",
    "density = nx.density(G)\n",
    "print(f'Network density: {density}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - Is the network fully connected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:42.615791200Z",
     "start_time": "2024-02-23T12:59:42.406350900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the network connected: False\n"
     ]
    }
   ],
   "source": [
    "# Network connection between nodes\n",
    "is_connected = nx.is_connected(G)\n",
    "print(f'Is the network connected: {is_connected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - If the network is disconnected, how many connected components does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:42.811268600Z",
     "start_time": "2024-02-23T12:59:42.612799600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of connected components: 68\n"
     ]
    }
   ],
   "source": [
    "# Number of connected components\n",
    "connected_components = nx.number_connected_components(G)\n",
    "print(f'Number of connected components: {connected_components}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - How many isolated nodes are there in your network?  An isolated node is defined as a node with no connections to any other node in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T12:59:42.894045600Z",
     "start_time": "2024-02-23T12:59:42.814260500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolated nodes: 0\n"
     ]
    }
   ],
   "source": [
    "# Number of isolated nodes\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "print(f'Number of isolated nodes: {len(isolated_nodes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why?  __(answer in max 150 words)__\n",
    "\n",
    "That the density is low would make sense, as you wouldn't expect every author to have co-authored with every other author, since we find works across different fields and the authors most likely would specify in just a couple of those. This also relates to connectivity, as this idea would create different groups in the network, thus not making it fully connected. We would also expect every author to have co-authored with at least one other author, so that there is no isolated nodes makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. **Degree Analysis:**\n",
    ">    - Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T13:11:18.989755700Z",
     "start_time": "2024-02-23T13:11:16.454533800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree Analysis:\n",
      "Average: 7.05\n",
      "Median: 5.0\n",
      "Mode: 4\n",
      "Minimum: 1\n",
      "Maximum: 1548\n",
      "\n",
      "Strength Analysis:\n",
      "Average: 8.48\n",
      "Median: 5.0\n",
      "Mode: 4\n",
      "Minimum: 1\n",
      "Maximum: 1763\n"
     ]
    }
   ],
   "source": [
    "# Degree\n",
    "degree = dict(G.degree())\n",
    "degree_values = list(degree.values())\n",
    "\n",
    "# Strength\n",
    "strength = dict(G.degree(weight='weight'))\n",
    "strength_values = list(strength.values())\n",
    "\n",
    "# Average\n",
    "avg_degree = np.mean(degree_values)\n",
    "avg_strength = np.mean(strength_values)\n",
    "\n",
    "# Median\n",
    "median_degree = np.median(degree_values)\n",
    "median_strength = np.median(strength_values)\n",
    "\n",
    "# Mode\n",
    "mode_degree = max(set(degree_values), key=degree_values.count)\n",
    "mode_strength = max(set(strength_values), key=strength_values.count)\n",
    "\n",
    "# Minimum\n",
    "min_degree = min(degree_values)\n",
    "min_strength = min(strength_values)\n",
    "\n",
    "# Maximum\n",
    "max_degree = max(degree_values)\n",
    "max_strength = max(strength_values)\n",
    "\n",
    "print(f'Degree Analysis:')\n",
    "print(f'Average: {avg_degree:.2f}')\n",
    "print(f'Median: {median_degree}')\n",
    "print(f'Mode: {mode_degree}')\n",
    "print(f'Minimum: {min_degree}')\n",
    "print(f'Maximum: {max_degree}')\n",
    "print(f'\\nStrength Analysis:')\n",
    "print(f'Average: {avg_strength:.2f}')\n",
    "print(f'Median: {median_strength}')\n",
    "print(f'Mode: {mode_strength}')\n",
    "print(f'Minimum: {min_strength}')\n",
    "print(f'Maximum: {max_strength}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What do these metrics tell us about the network? __(answer in max 150 words)__\n",
    "\n",
    "These metrics tell us that the median author (degree and strength analysis) has collaborated with 5 other coauthors. As expected we also see that when we do the strength analysis the average rises. This is because the strength is the weighted degree, and as such the more papers an author has written with another author, the higher the strength. The maximum also tells us that there's an author which has coauthored 1763 articles under the filters we've used. This high number also skews the average. This author is known as a 'hub' and compared to the relatively low median this shows the exponential decay which is typical for real networks. Also the strength analysis is very similar to Science Collaboration network in  table 2.1 in the Network Science book. This further substantiates the universality of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. **Top Authors:**\n",
    ">    - Identify the top 5 authors by degree. What role do these node play in the network?\n",
    "\n",
    "The top five authors are found below and displayed together with their degree value.\n",
    "In the network they have a role of being a connector between many different notes. As their degree is high, thay have many connection to other notes, thus meaning they have high contribution to writing different works with a varity of author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T14:45:46.258641Z",
     "start_time": "2024-02-23T14:45:46.130635600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://openalex.org/A5027835055', 1548),\n",
       " ('https://openalex.org/A5071773009', 898),\n",
       " ('https://openalex.org/A5070812231', 762),\n",
       " ('https://openalex.org/A5073501391', 745),\n",
       " ('https://openalex.org/A5036726873', 613)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the top 5authors by degree.\n",
    "top_5_degree = sorted(degree.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_5_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    - Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? __(answer in max 150 words)__\n",
    "\n",
    "The top five authors found are named Jun Li, Yu Zhang, Rui Wang, Jie Zhang, and Meng Wang. While they have work counts between 10.000 and 34.000, these authors still appear in the network, as they are co-authors of papers found in the last step of the data collection process and thus wasn't filtered away even though they have done more than 5.000 works.\n",
    "The authors seem to specialize in fields different from Computational Social Science as their recent works focus on different biotechnical aspects varying from the human body, ethnopharmacology, and antibiotics. These don't match with the concepts we tried to filter by when retrieving the works. One possible reason for this could be that an author helped with the scientific aspects of a work and thus were rightfully credited. However, this will lead us to authors that don't necessarily study in either of the categories we tried to capture."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
