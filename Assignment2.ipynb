{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github Repository\n",
    "https://github.com/MathiasDamsgaard/Comp_Social_Sci_Assigments.git\n",
    "\n",
    "### Contribution statement\n",
    "We all helped each other with the different parts of the assignment, as we all sat toghether when reviewing the exercises from the first four weeks. While Andreas finished up part 1, Mathias corrected the answers for part 2, and Anton added the code for part 3 and part 4, before we all looked thorugh each others work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "import time\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Properties of the real-world network of Computational Social Scientists\n",
    "\n",
    "These exercises are taken from Week 5.\n",
    "> __Exercise: Analyzing Networks through a Random Model__ \n",
    ">\n",
    "> 1. _Random Network_: Let's start by building a Random Network, acting as a baseline (or [\"null model\"](https://en.wikipedia.org/wiki/Null_model)) to understand the Computational Social Scientists Network better.  \n",
    "> * First, calculate the probability (_p_) that makes the expected number of edges in our random network match the actual edge count in the Computational Social Scientists network. Refer to equation 3.2 in your Network Science textbook for guidance. After finding _p_, figure out the average degree (using the given formula). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Now, write a function to generate a Random Network that mirrors the Computational Social Scientists network in terms of node count, using your calculated _p_. Generate a random network by linking nodes in every possible pair with probability _p_. **Hint**: you can use the function [``np.random.uniform``](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) to draw samples from a uniform probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Visualize the network as you did for the Computational Social Scientists network in the exercise above (my version is below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Answer the following questions __(max 200 words in total)__: \n",
    ">    - What regime does your random network fall into? Is it above or below the critical threshold?  \n",
    ">    - According to the textbook, what does the network's structure resemble in this regime?  \n",
    ">    - Based on your visualizations, identify the key differences between the actual and the random networks. Explain whether these differences are consistent with theoretical expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> 2. _Degree Distribution_: This section focuses on analyzing the degree distribution of both the actual Computational Social Scientists network and its random counterpart.\n",
    "> * Compute the distribution of degree for the random network using the numpy function ``np.histogram``. Choose bins, and normalization strategies appropriately. **Hint:** Revisit the content from Week 3, Part 3 of the lectures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Compute the distribution of degree for the Computational Social Scientists network using the numpy function ``np.histogram``. Also here, choose bins and normalization strategies wisely, based on the recommendations from previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Plot both degree distributions on the same figure using line plots, ensuring the x and y axes are scaled in a way that allows for comparison between the two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Add two vertical lines showing the average degree for the random and the real network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Answer the following questions __(max 200 words in total)__: \n",
    ">    - Does the average degree meaningfully represents the network's characteristics, especially in light of the insights gained from exploring heavy-tailed distributions? Discuss its adequacy or limitations in capturing the essence of the network's structural properties. \n",
    ">    - What differences can you observe between the real and the random distributions? How does the shape of the degree distribution for each network inform us about the network's properties? (max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Network Analysis in Computational Social Science\n",
    "\n",
    "These exercises are taken from Week 6. \n",
    "\n",
    "> __Exercise 1: Mixing Patterns and Assortativity__ \n",
    ">\n",
    "> __Part 1: Assortativity Coefficient__ \n",
    "> 1. *Calculate the Assortativity Coefficient* for the network based on the country of each node. Implement the calculation using the formula provided during the lecture, also available in [this paper](https://arxiv.org/pdf/cond-mat/0209450.pdf) (equation 2, here for directed networks). **Do not use the NetworkX implementation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def assortativity(G, attribute):\n",
    "    # Get number of nodes of attribute i that connect to nodes of attribute j (eg. E_ij, E_ii, E_jj)\n",
    "    E = np.zeros((len(set(nx.get_node_attributes(G, attribute).values())), len(set(nx.get_node_attributes(G, attribute).values()))))\n",
    "    # use tqdm to show progress bar\n",
    "    for node in G.nodes:\n",
    "        for neighbor in G.neighbors(node):\n",
    "            E[list(set(nx.get_node_attributes(G, attribute).values())).index(G.nodes[node][attribute]), list(set(nx.get_node_attributes(G, attribute).values())).index(G.nodes[neighbor][attribute])] += 1\n",
    "\n",
    "    # Sum of E_ii\n",
    "    E_ii = np.sum(np.diag(E))\n",
    "    # The square of the sum of E_ij (where i != j)\n",
    "    E_ij2 = np.sum(E ** 2) - E_ii**2\n",
    "    \n",
    "    r = (E_ii - E_ij2) / (1 - E_ij2)\n",
    "    \n",
    "    return r, E"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "> __Part 2: Configuration model__\n",
    "> In the following, we are going to assess the significance of the assortativity by comparing the network's assortativity coefficient against that of random networks generated through the configuration model.  \n",
    ">\n",
    "> 2. *Implement the configuration model* using the _double edge swap_ algorithm to generate random networks. Ensure each node retains its original degree but with altered connections. Create a function that does that by following these steps:\n",
    ">   - **a.** Create an exact copy of your original network.\n",
    ">   - **b.** Select two edges, $e_{1} = (u,v)$ and $e_{2} = (x,y)$, ensuring *u != y* and *v != x*.\n",
    ">   - **c.** Flip the direction of $e_{1}$ to $e_{1} = (v,u)$ 50% of the time. This ensure that your final results is not biased, in case your edges were sorted (they usually are). \n",
    ">   - **d.** Ensure that new edges $e_{1}' = (e_{1}[0],e_{2}[1])$ and $e_{2}' = (e_{2}[0],e_{1}[1])$ do not already exist in the network.\n",
    ">   - **e.** Remove edges $e_{1}$ and $e_{2}$ and add edges $e_{1}'$ and $e_{2}'$.\n",
    ">   - **f.** Repeat steps **b** to **e** until you have performed _E*10_ swaps, where E is the total number of edges."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def configuration_model(G: nx.Graph):\n",
    "    G_copy = G.copy()\n",
    "    edges = list(G_copy.edges())\n",
    "    num_swaps = 10 * G_copy.number_of_edges()\n",
    "\n",
    "    for _ in tqdm(range(num_swaps)):\n",
    "        # b. Select two edges\n",
    "        e1, e2 = random.sample(edges, 2)\n",
    "\n",
    "        # Ensure distinct nodes\n",
    "        if len(set(e1 + e2)) < 4:\n",
    "            continue\n",
    "\n",
    "        # c. Flip the direction of e1 50% of the time\n",
    "        if random.random() < 0.5:\n",
    "            e1 = (e1[1], e1[0])\n",
    "\n",
    "        # d. Create new edges\n",
    "        e1_new = (e1[0], e2[1])\n",
    "        e2_new = (e2[0], e1[1])\n",
    "\n",
    "        # e. Remove old edges and add new edges\n",
    "        G_copy.remove_edges_from([e1, e2])\n",
    "        G_copy.add_edges_from([e1_new, e2_new])\n",
    "        \n",
    "\n",
    "    return G_copy\n",
    "\n",
    "G_config = configuration_model(G)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:09:53.868494Z",
     "start_time": "2024-03-20T13:09:53.861494Z"
    }
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "> 3. *Double check that your algorithm works well*, by showing that the degree of nodes in the original network and the new 'randomized' version of the network are the same."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check that the degree of nodes in the original network and the new 'randomized' version of the network are the same\n",
    "assert all([G.degree(node) == G_config.degree(node) for node in G.nodes])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:09:53.915495Z",
     "start_time": "2024-03-20T13:09:53.901495Z"
    }
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "> __Part 3: Analyzing Assortativity in Random Networks__  \n",
    ">\n",
    "> 4. *Generate and analyze at least 100 random networks* using the configuration model. For each, calculate the assortativity with respect to the country and plot the distribution of these values. Compare the results with the assortativity of your original network to determine if connections within the same country are significantly higher than chance.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assortativities = []\n",
    "for _ in range(100):\n",
    "    G_config = configuration_model(G)\n",
    "    r, _ = assortativity(G_config, 'group')\n",
    "    assortativities.append(r)\n",
    "\n",
    "# Plot the distribution of the assortativities\n",
    "pd.Series(assortativities).hist()\n",
    "\n",
    "# Calculate the assortativity of the original network\n",
    "r, _ = assortativity(G, 'group')\n",
    "\n",
    "# Plot the assortativity of the original network\n",
    "plt.axvline(r, color='r')\n",
    "plt.xlabel('Assortativity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Assortativity of the original network')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:09:53.930496Z",
     "start_time": "2024-03-20T13:09:53.924495Z"
    }
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "> __Part 4: Assortativity by Degree__\n",
    ">\n",
    "> 5. *Calculate degree assortativity* for your network using the formula discussed in the lecture.\n",
    "> 6. *Compare your network's degree assortativity* against that of 100 random networks generated via the configuration model. Analyze whether your network shows a tendency for high-degree scientists to connect with other high-degree scientists and vice versa. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Lav funktion til at beregne degree assort\n",
    "print(nx.degree_assortativity_coefficient(G))\n",
    "\n",
    "# Compare to random networks\n",
    "assortativities = []\n",
    "for _ in range(100):\n",
    "    G_config = configuration_model(G)\n",
    "    r = nx.degree_assortativity_coefficient(G_config)\n",
    "    assortativities.append(r)\n",
    "print(assortativities)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:09:53.945493Z",
     "start_time": "2024-03-20T13:09:53.938493Z"
    }
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    ">\n",
    "> __Part 5: Reflection questions (max 250 words for the 3 questions)__    \n",
    "> 7. *Assortativity by degree.* Were the results of the degree assortativity in line with your expectations? Why or why not?    \n",
    "> 8. *Edge flipping.* In the process of implementing the configuration model, you were instructed to flip the edges (e.g., changing *e_1* from (u,v) to (v,u)) 50% of the time. Why do you think this step is included?    \n",
    "> 9. *Distribution of assortativity in random networks.* Describe the distribution of degree assortativity values you observed for the random networks. Was the distribution pattern expected? Discuss how the nature of random network generation (specifically, the configuration model and edge flipping) might influence this distribution and whether it aligns with theoretical expectations.    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVAR:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "> **Exercise 2: Central nodes.** Remember to write your answers in the notebook. \n",
    "> * Find the 5 most central scientists according to the closeness centrality. What role do you imagine scientists with high closeness centrality play? \n",
    "> * Find the 5 most central scientists according to eigenvector centrality.     \n",
    "> * Plot the closeness centrality of nodes vs their degree. Is there a correlation between the two? Did you expect that? Why?    \n",
    "> * Repeat the two points above using eigenvector centrality instead. Do you observe any difference? Why?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'The 5 most central scientists according to closeness centrality are: {sorted_closeness_centrality[:5]}')\n",
    "# Find the 5 most central scientists according to eigenvector centrality.     \n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'The 5 most central scientists according to eigenvector centrality are: {sorted_eigenvector_centrality[:5]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "degree = dict(G.degree())\n",
    "plt.scatter(list(closeness_centrality.values()), list(degree.values()))\n",
    "plt.xlabel('Closeness centrality')\n",
    "plt.ylabel('Degree')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "degree = dict(G.degree())\n",
    "plt.scatter(list(eigenvector_centrality.values()), list(degree.values()))\n",
    "plt.xlabel('Eigenvector centrality')\n",
    "plt.ylabel('Degree')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVAR: "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Words that characterize Computational Social Science communities\n",
    "\n",
    "These exercises are taken from Week 8\n",
    "> __Exercise 1: TF-IDF and the Computational Social Science communities.__ The goal for this exercise is to find the words charachterizing each of the communities of Computational Social Scientists.\n",
    "> What you need for this exercise: \n",
    ">    * The assignment of each author to their network community, and the degree of each author (Week 6, Exercise 4). This can be stored in a dataframe or in two dictionaries, as you prefer.  \n",
    ">    * the tokenized _abstract_ dataframe (Week 7, Exercise 2)\n",
    ">\n",
    "> 1. First, check out [the wikipedia page for TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Explain in your own words the point of TF-IDF. \n",
    ">   * What does TF stand for? \n",
    ">   * What does IDF stand for?\n",
    "> 2. Now, we want to find out which words are important for each *community*, so we're going to create several ***large documents, one for each community***. Each document includes all the tokens of abstracts written by members of a given community. \n",
    ">   * Consider a community _c_\n",
    ">   * Find all the abstracts of papers written by a member of community _c_.\n",
    ">   * Create a long array that stores all the abstract tokens \n",
    ">   * Repeat for all the communities. \n",
    "> __Note:__ Here, to ensure your code is efficient, you shall exploit ``pandas`` builtin functions, such as [``groupby.apply``](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.apply.html) or [``explode``](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html).\n",
    "> 3. Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within the __top 5 communities__ (by number of authors). \n",
    ">   * Describe similarities and differences between the communities.\n",
    ">   * Why aren't the TFs not necessarily a good description of the communities?\n",
    ">   * Next, we calculate IDF for every word. \n",
    ">   * What base logarithm did you use? Is that important?\n",
    "> 4. We're ready to calculate TF-IDF. Do that for the __top 9 communities__ (by number of authors). Then for each community: \n",
    ">   * List the 10 top TF words \n",
    ">   * List the 10 top TF-IDF words\n",
    ">   * List the top 3 authors (by degree)\n",
    ">   * Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T13:09:54.806494Z",
     "start_time": "2024-03-20T13:09:54.792528Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Exercise 2: The Wordcloud__. It's time to visualize our results!\n",
    "\n",
    "> * Install the [`WordCloud`](https://pypi.org/project/wordcloud/) module. \n",
    "> * Now, create word-cloud for each community. Feel free to make it as fancy or non-fancy as you like.\n",
    "> * Make sure that, together with the word cloud, you print the names of the top three authors in each community (see my plot above for inspiration). \n",
    "> * Comment on your results. What can you conclude on the different sub-communities in Computational Social Science? \n",
    "> * Look up online the top author in each community. In light of your search, do your results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T13:09:55.668493Z",
     "start_time": "2024-03-20T13:09:55.657494Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Exercise 3: Computational Social Science__ \n",
    "\n",
    "> * Go back to Week 1, Exercise 1. Revise what you wrote on the topics in Computational Social Science. \n",
    "> * In light of your data-driven analysis, has your understanding of the field changed? How? __(max 150 words)__"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
