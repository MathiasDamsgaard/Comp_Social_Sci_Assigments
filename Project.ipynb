{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Union, Optional\n",
    "# Modules\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xmlrpc.client as xc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set client to the PyPI XML-RPC server\n",
    "client = xc.ServerProxy('http://pypi.python.org/pypi')\n",
    "\n",
    "# Get a list of all the packages\n",
    "pypi_packages = client.list_packages()\n",
    "\n",
    "# lowercase all the package names\n",
    "pypi_packages = [package.lower() for package in pypi_packages]\n",
    "\n",
    "# Save the list of packages\n",
    "with open(\"data/packages.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pypi_packages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_github_link(packages: list) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a list of python packages and returns a list of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \n",
    "    list_of_packages: list\n",
    "        List of python packages to search for.\n",
    "        \n",
    "    return: list\n",
    "        List of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_links = []\n",
    "    for i, package in enumerate(packages):\n",
    "        # The link to the python package\n",
    "        LINK = f\"https://pypi.org/project/{package}/\"\n",
    "        \n",
    "        # Get the HTML content of the page\n",
    "        r = requests.get(LINK)\n",
    "        \n",
    "        # If the request was not successful, alert the user\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Request failed for {package, i}: {r.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        \n",
    "        # Get sidebar with links\n",
    "        sidebar = soup.find(\"div\", {\"class\": \"vertical-tabs__tabs\"})\n",
    "        \n",
    "        # Get all the links in the sidebar\n",
    "        references = [link.get(\"href\") for link in sidebar.find_all(\"a\")]\n",
    "        \n",
    "        # Join into one string to regex in\n",
    "        reference_text = \" \".join([reference for reference in references if reference is not None])\n",
    "        \n",
    "        # Find the first link that contains the word \"github.com\"\n",
    "        github_links = []\n",
    "        for link in re.finditer(r\"github\\.com(/\\w*|/\\W|[-]\\w*|[-]\\W*)*\", reference_text):\n",
    "            if link.group() != \"github.com/\" and link.group() != \"github.com\":\n",
    "                github_links.append(link.group())\n",
    "        \n",
    "        # If there are no links, append None\n",
    "        if len(github_links) == 0:\n",
    "            github_link = None\n",
    "        \n",
    "        # If there's several take the shortest and alert the user\n",
    "        elif len(github_links) > 1:\n",
    "            print(f\"Several GitHub links found for {package, i}: {github_links}\")\n",
    "            github_link = min(github_links, key=len)\n",
    "        \n",
    "        # If there is just one link, take that out of the list\n",
    "        elif len(github_links) == 1:\n",
    "            github_link = github_links[0]\n",
    "        \n",
    "        # Else alert the user no githublink is found\n",
    "        else:\n",
    "            print(f\"No GitHub link found for {package, i}\")\n",
    "            github_link = None\n",
    "        \n",
    "        # Append the triplet to the list\n",
    "        all_links.append((package, LINK, github_link))\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "\n",
    "# Run the function with threadpool executor to speed up the process - still takes a loooong time so be aware\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    all_links = list(tqdm(executor.map(get_github_link, [pypi_packages]), total=len(pypi_packages)))\n",
    "\n",
    "# Save the list to a json file\n",
    "with open(\"data/all_links_github.json\", \"w\") as f:\n",
    "    json.dump(all_links, f)\n",
    "\n",
    "# Clean the list of None links\n",
    "all_links_c = [(p, l, g) for p, l, g in all_links if g is not None]\n",
    "\n",
    "with open(\"data/all_links_github_c.json\", \"w\") as f:\n",
    "    json.dump(all_links_c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "print(\"Number of packages on pypi:\", len(pypi_packages))\n",
    "\n",
    "with open('data/all_links_github.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of packages to successfully access the webpage:\", len(data))\n",
    "\n",
    "with open('data/all_links_github_c.json', 'r') as f:\n",
    "    data_clean = json.load(f)\n",
    "print(\"Number of packages to successfully get the github link from:\", len(data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each package go to the GitHub page and get the readme.text if theres a README.md\n",
    "def get_readme_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the README.md file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the README.md file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{github_link}/main/README.md\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/master/README.md\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/main/REAMDE.rst\")\n",
    "                if response.status_code != 200:\n",
    "                    response = requests.get(f\"{github_link}/master/REAMDE.rst\")\n",
    "                    if response.status_code != 200:\n",
    "                        response = requests.get(f\"{github_link}/main/README.txt\")\n",
    "                        if response.status_code != 200:\n",
    "                            response = requests.get(f\"{github_link}/master/README.txt\")\n",
    "                            if response.status_code != 200:\n",
    "                                return None\n",
    "                            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    readme_text = response.text\n",
    "\n",
    "    # Remove links which start with http\n",
    "    readme_text = re.sub(r\"http.*\", \"\", readme_text)\n",
    "    # Remove links to files in the repository which start with / or ./ or ../\n",
    "    readme_text = re.sub(r\"/.*|./.*|../.*\", \"\", readme_text)\n",
    "    # Convert /n to space\n",
    "    readme_text = re.sub(r\"\\n\", \" \", readme_text)\n",
    "    # Make all text lowercase\n",
    "    readme_text = readme_text.lower()\n",
    "    # Only keep Alphanumeric characters and - and _\n",
    "    readme_text = re.sub(r\"[^a-z0-9-_ ]\", \"\", readme_text)\n",
    "    # Remove multiple spaces\n",
    "    readme_text = re.sub(r\" +\", \" \", readme_text)\n",
    "    # Remove empty strings\n",
    "    readme_text = [line for line in readme_text.split(\" \") if line != \"\"]\n",
    "\n",
    "    return readme_text\n",
    "\n",
    "\n",
    "def get_requirements_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the requirements.txt file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the requirements.txt file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "\n",
    "    txt_bool = True\n",
    "    pyproject_bool = False\n",
    "\n",
    "    try:\n",
    "        response = requests.get(f\"{github_link}/main/requirements-dev.txt\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/master/requirements-dev.txt\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/main/dev-requirements.txt\")\n",
    "                if response.status_code != 200:\n",
    "                    response = requests.get(f\"{github_link}/master/dev-requirements.txt\")\n",
    "                    if response.status_code != 200:\n",
    "                        txt_bool = False\n",
    "                        response = requests.get(f\"{github_link}/main/environment.yml\")\n",
    "                        if response.status_code != 200:\n",
    "                            response = requests.get(f\"{github_link}/master/environment.yml\")\n",
    "                            if response.status_code != 200:\n",
    "                                pyproject_bool = True\n",
    "                                response = requests.get(f\"{github_link}/main/pyproject.toml\")\n",
    "                                if response.status_code != 200:\n",
    "                                    response = requests.get(f\"{github_link}/master/pyproject.toml\")\n",
    "                                    if response.status_code != 200:\n",
    "                                        pyproject_bool = False\n",
    "                                        txt_bool = True\n",
    "                                        response = requests.get(f\"{github_link}/main/requirements.txt\")\n",
    "                                        if response.status_code != 200:\n",
    "                                            response = requests.get(f\"{github_link}/master/requirements.txt\")\n",
    "                                            if response.status_code != 200:\n",
    "                                                response = requests.get(f\"{github_link}/main/requirements.txt\")\n",
    "                                                if response.status_code != 200:\n",
    "                                                    return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None                                \n",
    "\n",
    "    requirements_text = response.text\n",
    "\n",
    "    # Clean the text using regex\n",
    "    cleaning_reg = r\"=.*|>.*|~.*|\\[.*\\]|;.*|<.*|!.*\"\n",
    "\n",
    "    if txt_bool:\n",
    "        # Example: \n",
    "            # versioneer[toml]\n",
    "            # cython~=3.0.5\n",
    "            # meson[ninja]==1.2.1\n",
    "            # meson-python==0.13.1\n",
    "            # pytest>=7.3.2\n",
    "            # pytest-cov\n",
    "            # pytest-xdist>=2.2.0\n",
    "            # pytest-qt>=4.2.0\n",
    "        # We only want the package name and not the version or extras\n",
    "        requirements_text = re.sub(r\"\\[.*\\]\", \"\", requirements_text)\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Clean the text using regex\n",
    "        requirements_text = re.sub(f\"{cleaning_reg}\", \"\", requirements_text)\n",
    "        # lower case\n",
    "        requirements_text = requirements_text.lower()\n",
    "        # Convert to list\n",
    "        requirements_text = requirements_text.split(\"\\n\")\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    elif pyproject_bool:\n",
    "        # Example:\n",
    "            # [project]\n",
    "            # name = \"pydata-sphinx-theme\"\n",
    "            # description = \"Bootstrap-based Sphinx theme from the PyData community\"\n",
    "            # readme = \"README.md\"\n",
    "            # requires-python = \">=3.9\"\n",
    "            # dependencies = [\n",
    "            # \"Babel\",\n",
    "            # \"pygments>=2.7\",\n",
    "            # \"accessible-pygments\",\n",
    "            # \"typing-extensions\"\n",
    "            # ]\n",
    "            # [project.optional-dependencies]\n",
    "            # doc = [\n",
    "            # \"numpydoc\",\n",
    "            # \"linkify-it-py\", # for link shortening\n",
    "            # \"rich\",\n",
    "            # # For examples section\n",
    "            # \"myst-parser\"\n",
    "            # ]\n",
    "\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        dependencies = re.findall(r'dependencies = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        optional_dependencies = re.findall(r'optional-dependencies\\]\\n.*? = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        if len(dependencies) == 0:\n",
    "            return None\n",
    "        if len(optional_dependencies) == 0:\n",
    "            optional_dependencies = [\"\"]\n",
    "        \n",
    "        dependencies = re.findall(r'\".*\"', dependencies[0])\n",
    "        optional_dependencies = re.findall(r'\".*\"', optional_dependencies[0])\n",
    "        requirements_text = dependencies + optional_dependencies\n",
    "        # Remove double quotes\n",
    "        requirements_text = [requirement[1:-1] for requirement in requirements_text]\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # lower case\n",
    "        requirements_text = [requirement.lower() for requirement in requirements_text]\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    else:\n",
    "        # Example:\n",
    "            # name: myenv\n",
    "            # channels:\n",
    "            #   - defaults\n",
    "            # dependencies:\n",
    "            #   - numpy\n",
    "            #   - pandas\n",
    "            #   - pip\n",
    "            #   - pip:\n",
    "            #     - matplotlib\n",
    "        \n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Only get the dependencies which start with '- '\n",
    "        requirements_text = re.findall(r\"- .*\", requirements_text)\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # lower case\n",
    "        requirements_text = [requirement.lower() for requirement in requirements_text]\n",
    "        # Convert to list\n",
    "        requirements_text = [requirement[2:] for requirement in requirements_text]\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "           \n",
    "    return requirements_text\n",
    "\n",
    "\n",
    "def node_creator(data: Tuple[str, str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Function that takes a list of tuples with the package name, the link to the PyPI page and the link to the GitHub page and returns a dictionary with the package name as the key and the value is a dictionary with the package name, the link to the PyPI page, the link to the GitHub page, the text of the README.md file and the text of the requirements.txt file.\n",
    "    \n",
    "    data: list\n",
    "        List of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "        \n",
    "    return: dict\n",
    "        Dictionary with the package name as the key and the value is a dictionary with the package name, the link to the PyPI page, the link to the GitHub page, the text of the README.md file and the text of the requirements.txt file.\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    \n",
    "    package, link, github_link = data\n",
    "\n",
    "    readme_text = get_readme_text(github_link)\n",
    "    requirements_text = get_requirements_text(github_link)\n",
    "    if requirements_text is None:\n",
    "        return None\n",
    "    node[package] = {\"package\": package, \"link\": link, \"github_link\": github_link, \"readme_text\": readme_text, \"requirements_text\": requirements_text}\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Test the function\n",
    "test_data = (\"numpy\", \"https://pypi.org/project/numpy/\", \"github.com/numpy/numpy\")\n",
    "test_node = node_creator(test_data)\n",
    "print(test_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "with open('data/all_links_github_c.json', 'r') as f:\n",
    "    data_clean = json.load(f)\n",
    "\n",
    "# Run the function with threadpool executor to speed up the process\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    nodes = list(tqdm(executor.map(node_creator, data_clean[110000:220000]), total=len(data_clean[110000:220000])))\n",
    "\n",
    "# Save the list to a json file\n",
    "with open(\"data/nodes_anton.json\", \"w\") as f:\n",
    "    json.dump(nodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS THE FINAL CLEANING OF THE DATA IF NEEDED ####\n",
    "with open('data/nodes_anton_clean.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "for node in data:\n",
    "    # lower the keys\n",
    "    node = {key.lower(): value for key, value in node.items()}\n",
    "    for key, value in node.items():\n",
    "        value[\"package\"] = value[\"package\"].lower()\n",
    "        value[\"requirements_text\"] = [requirement.strip() for requirement in value[\"requirements_text\"]]\n",
    "        value[\"requirements_text\"] = [requirement for requirement in value[\"requirements_text\"] if requirement != \"\"]\n",
    "        value[\"requirements_text\"] = [requirement.lower() for requirement in value[\"requirements_text\"]]\n",
    "        value[\"requirements_text\"] = [re.sub(r\"==.*|>=.*|<=.*|~=.*|!=.*|>.*|<.*\", \"\", requirement) for requirement in value[\"requirements_text\"]]\n",
    "\n",
    "    cleaned_data.append(node)\n",
    "\n",
    "\n",
    "with open(\"data/nodes_anton_clean.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10524/10524 [03:58<00:00, 44.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of packages not in PyPI: 1735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biotracks', 'datapackage'),\n",
       " ('biotracks', 'jsontableschema'),\n",
       " ('biotracks', 'jsontableschema-pandas'),\n",
       " ('windio', 'defaults'),\n",
       " ('windio', 'jsonschema'),\n",
       " ('windio', 'numpy'),\n",
       " ('windio', 'pyyaml'),\n",
       " ('windio', 'pytest'),\n",
       " ('windio', 'xarray'),\n",
       " ('python-etcd-lock', 'nose')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we make the edgelist \n",
    "# Load the data\n",
    "with open('data/nodes_anton_clean.json', 'r') as f:\n",
    "    nodes = json.load(f)\n",
    "\n",
    "edge_list = []\n",
    "\n",
    "packages_not_in_pypi = set()\n",
    "\n",
    "for node in tqdm(nodes):\n",
    "    if node is None:\n",
    "        continue\n",
    "    for package in node:\n",
    "        if node[package][\"requirements_text\"] is None:\n",
    "            continue\n",
    "        for requirement in node[package][\"requirements_text\"]:\n",
    "            if requirement not in pypi_packages:\n",
    "                packages_not_in_pypi.add(requirement)\n",
    "                continue\n",
    "            edge_list.append((package, requirement))\n",
    "\n",
    "print(\"Number of packages not in PyPI:\", len(packages_not_in_pypi))\n",
    "\n",
    "# Save the edge list\n",
    "with open(\"data/edge_list_anton.pkl\", \"wb\") as f:\n",
    "    pickle.dump(edge_list, f)\n",
    "\n",
    "edge_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'package': 'windio',\n",
       " 'link': 'https://pypi.org/project/windIO/',\n",
       " 'github_link': 'github.com/IEAWindTask37/windIO',\n",
       " 'readme_text': ['build',\n",
       "  'status',\n",
       "  'documentation',\n",
       "  'status',\n",
       "  'windio',\n",
       "  'frameworks',\n",
       "  'defining',\n",
       "  'the',\n",
       "  'inputs',\n",
       "  'and',\n",
       "  'outputs',\n",
       "  'for',\n",
       "  'systems',\n",
       "  'engineering',\n",
       "  'mdao',\n",
       "  'of',\n",
       "  'wind',\n",
       "  'turbine',\n",
       "  'and',\n",
       "  'plants',\n",
       "  'the',\n",
       "  'framework',\n",
       "  'was',\n",
       "  'developed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'iea',\n",
       "  'wind',\n",
       "  'task',\n",
       "  '37',\n",
       "  'team',\n",
       "  'within',\n",
       "  'work',\n",
       "  'package',\n",
       "  '1',\n",
       "  'author',\n",
       "  'iea',\n",
       "  'wind',\n",
       "  'task',\n",
       "  '37',\n",
       "  'teammailtopietrobortolottinrelgov',\n",
       "  'version',\n",
       "  'this',\n",
       "  'software',\n",
       "  'is',\n",
       "  'a',\n",
       "  'version',\n",
       "  '10',\n",
       "  'documentation',\n",
       "  'and',\n",
       "  'citation',\n",
       "  'the',\n",
       "  'online',\n",
       "  'documentation',\n",
       "  'can',\n",
       "  'be',\n",
       "  'accessed',\n",
       "  'here',\n",
       "  'if',\n",
       "  'you',\n",
       "  'use',\n",
       "  'this',\n",
       "  'model',\n",
       "  'in',\n",
       "  'your',\n",
       "  'research',\n",
       "  'or',\n",
       "  'publications',\n",
       "  'please',\n",
       "  'cite',\n",
       "  'this',\n",
       "  'iea',\n",
       "  'technical',\n",
       "  'report',\n",
       "  'articleosti_1868328',\n",
       "  'title',\n",
       "  'system',\n",
       "  'modeling',\n",
       "  'frameworks',\n",
       "  'for',\n",
       "  'wind',\n",
       "  'turbines',\n",
       "  'and',\n",
       "  'plants',\n",
       "  'review',\n",
       "  'and',\n",
       "  'requirements',\n",
       "  'specifications',\n",
       "  'author',\n",
       "  'bortolotti',\n",
       "  'pietro',\n",
       "  'and',\n",
       "  'bay',\n",
       "  'christopher',\n",
       "  'and',\n",
       "  'barter',\n",
       "  'garrett',\n",
       "  'and',\n",
       "  'gaertner',\n",
       "  'evan',\n",
       "  'and',\n",
       "  'dykes',\n",
       "  'katherine',\n",
       "  'and',\n",
       "  'mcwilliam',\n",
       "  'michael',\n",
       "  'and',\n",
       "  'friis-moller',\n",
       "  'mikkel',\n",
       "  'and',\n",
       "  'molgaard',\n",
       "  'pedersen',\n",
       "  'mads',\n",
       "  'and',\n",
       "  'zahle',\n",
       "  'frederik',\n",
       "  'doi',\n",
       "  '1021',\n",
       "  'place',\n",
       "  'united',\n",
       "  'states',\n",
       "  'year',\n",
       "  '2022',\n",
       "  'month',\n",
       "  '5',\n",
       "  'example',\n",
       "  'and',\n",
       "  'unit',\n",
       "  'testing',\n",
       "  'an',\n",
       "  'example',\n",
       "  'of',\n",
       "  'a',\n",
       "  'turbine',\n",
       "  'yaml',\n",
       "  'file',\n",
       "  'can',\n",
       "  'be',\n",
       "  'found',\n",
       "  'here',\n",
       "  'te',\n",
       "  'the',\n",
       "  'file',\n",
       "  'is',\n",
       "  'validated',\n",
       "  'using',\n",
       "  'the',\n",
       "  'jsonschema',\n",
       "  'library'],\n",
       " 'requirements_text': ['conda-forge',\n",
       "  'defaults',\n",
       "  'jsonschema',\n",
       "  'numpy',\n",
       "  'pyyaml',\n",
       "  'pytest',\n",
       "  'xarray']}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of dictionaries to a dictionary\n",
    "nodes_dict = {}\n",
    "for node in nodes:\n",
    "    if node is None:\n",
    "        continue\n",
    "    for key, value in node.items():\n",
    "        nodes_dict[key] = value\n",
    "\n",
    "nodes_dict['windio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 16146\n",
      "Number of edges: 67881\n",
      "Number of connected components: 172\n"
     ]
    }
   ],
   "source": [
    "# Load the edge list\n",
    "with open(\"data/edge_list_anton.pkl\", \"rb\") as f:\n",
    "    edge_list = pickle.load(f)\n",
    "    \n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edge_list)\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "print(\"Number of connected components:\", nx.number_connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the largest connected component: 15750\n"
     ]
    }
   ],
   "source": [
    "# Get the largest connected component\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "print(\"Number of nodes in the largest connected component:\", G.number_of_nodes())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
