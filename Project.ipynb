{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pypi dependency\n",
    "\n",
    "### Github Repository\n",
    "A link to the repository is avaliable [here](https://github.com/MathiasDamsgaard/Comp_Social_Sci_Assigments.git).\n",
    "\n",
    "### Contribution statement\n",
    "We all helped each other with the different parts of the project. For choosing the topic and collecting the data we all helped and contributed equally. Later on, Anton took responsability for the network analysis, Andreas then stood for the textual analysis, and Mathias for the webpage and overall layout. Everyone still helped working on every part to properly agree on the final conclusions.\n",
    "\n",
    "### Webpage\n",
    "You might have to run and create the webpage first, but afterwards this [link](http://localhost:1313/Comp_Social_Sci_Assigment_B/) should work.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "What is your dataset?\n",
    "\n",
    "This project works to investigate a network constructed of _python packages' dependencies_ on each other. This is based on the available packages on [python package index (pypi)](https://pypi.org/). Inspiration of this project idea comes from seeing an older dataset on [Netzschleuder](https://networks.skewed.de/net/python_dependency) made by Kevin Gullikson. His [original blogpost](https://kgullikson88.github.io/blog/pypi-analysis.html) is from 2016, and will therefore be used as a baseline for comparing the networks on a timescale.\n",
    "\n",
    "Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "We found it interesting to research how the package library in Python has evolved over the years and further investigate which packages seem to be the father/mother of all packages. Creating libraries is a big part of programming to help make functions easier to use across files and users, and thus we hope we also will gain insight into how these libraries potentially fall into different groups to see in which fields users most often create new libraries.\n",
    "By also analysing the text available in README files, we hope to gain insight into how programmers formulate themselves when writing documentation for their code, and if it varies across different groups.\n",
    "\n",
    "What was your goal for the end user's experience? The end user is someone who looks at your website. What do you want them to learn from your analysis?\n",
    "\n",
    "Hopefully will the end user experience a breath of fresh air when reading through our analysis. It is a different and not often seen type of network, so we would like it, if the user found our research interesting not only from a computer science point of view, but also from a social scientify angle. While the project is inspiried by an interest for the idea of how the libraries will link togehter, it provides more insight into the habits of programmers, and it is this idea that we hope the end user will also learn something about, while reading through our website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Union, Optional, Callable\n",
    "# Modules\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xmlrpc.client as xc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import netwulf as wulf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import community as community_louvain\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import bigrams as make_bigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics.association import BigramAssocMeasures\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import wordcloud as wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acqusition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set client to the PyPI XML-RPC server\n",
    "client = xc.ServerProxy('http://pypi.python.org/pypi')\n",
    "\n",
    "# Get a list of all the packages\n",
    "pypi_packages = client.list_packages()\n",
    "\n",
    "# lowercase all the package names\n",
    "pypi_packages = [package.lower() for package in pypi_packages]\n",
    "\n",
    "# Save the list of packages\n",
    "with open(\"data/packages.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pypi_packages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_github_link(packages: list) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a list of python packages and returns a list of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \n",
    "    list_of_packages: list\n",
    "        List of python packages to search for.\n",
    "        \n",
    "    return: list\n",
    "        List of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_links = []\n",
    "    for i, package in enumerate(packages):\n",
    "        # The link to the python package\n",
    "        LINK = f\"https://pypi.org/project/{package}/\"\n",
    "        \n",
    "        # Get the HTML content of the page\n",
    "        r = requests.get(LINK)\n",
    "        \n",
    "        # If the request was not successful, alert the user\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Request failed for {package, i}: {r.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        \n",
    "        # Get sidebar with links\n",
    "        sidebar = soup.find(\"div\", {\"class\": \"vertical-tabs__tabs\"})\n",
    "        \n",
    "        # Get all the links in the sidebar\n",
    "        references = [link.get(\"href\") for link in sidebar.find_all(\"a\")]\n",
    "        \n",
    "        # Join into one string to regex in\n",
    "        reference_text = \" \".join([reference for reference in references if reference is not None])\n",
    "        \n",
    "        # Find the first link that contains the word \"github.com\"\n",
    "        github_links = []\n",
    "        for link in re.finditer(r\"github\\.com(/\\w*|/\\W|[-]\\w*|[-]\\W*)*\", reference_text):\n",
    "            if link.group() != \"github.com/\" and link.group() != \"github.com\":\n",
    "                github_links.append(link.group())\n",
    "        \n",
    "        # If there are no links, append None\n",
    "        if len(github_links) == 0:\n",
    "            github_link = None\n",
    "        \n",
    "        # If there's several take the shortest and alert the user\n",
    "        elif len(github_links) > 1:\n",
    "            print(f\"Several GitHub links found for {package, i}: {github_links}\")\n",
    "            github_link = min(github_links, key=len)\n",
    "        \n",
    "        # If there is just one link, take that out of the list\n",
    "        elif len(github_links) == 1:\n",
    "            github_link = github_links[0]\n",
    "        \n",
    "        # Else alert the user no githublink is found\n",
    "        else:\n",
    "            print(f\"No GitHub link found for {package, i}\")\n",
    "            github_link = None\n",
    "        \n",
    "        # Append the triplet to the list\n",
    "        all_links.append((package, LINK, github_link))\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "\n",
    "# Run the function with threadpool executor to speed up the process - still takes a loooong time so be aware\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    all_links = list(tqdm(executor.map(get_github_link, [pypi_packages]), total=len(pypi_packages)))\n",
    "\n",
    "# Clean the list of None links\n",
    "all_links = [(p, l, g) for p, l, g in all_links if g is not None]\n",
    "\n",
    "# Save the list of links\n",
    "with open(\"data/all_links_github.json\", \"w\") as f:\n",
    "    json.dump(all_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "print(\"Number of packages on pypi:\", len(pypi_packages))\n",
    "\n",
    "with open('data/all_links_github.json', 'r') as f:\n",
    "    data_clean = json.load(f)\n",
    "print(\"Number of packages to successfully get the github link from:\", len(data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each package go to the GitHub page and get the readme.text if theres a README.md\n",
    "def get_readme_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the README.md file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the README.md file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{github_link}/main/README.md\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/master/README.md\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/main/REAMDE.rst\")\n",
    "                if response.status_code != 200:\n",
    "                    response = requests.get(f\"{github_link}/master/REAMDE.rst\")\n",
    "                    if response.status_code != 200:\n",
    "                        response = requests.get(f\"{github_link}/main/README.txt\")\n",
    "                        if response.status_code != 200:\n",
    "                            response = requests.get(f\"{github_link}/master/README.txt\")\n",
    "                            if response.status_code != 200:\n",
    "                                return None\n",
    "                            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    readme_text = response.text\n",
    "\n",
    "    # Remove links which start with http\n",
    "    readme_text = re.sub(r\"http.*\", \"\", readme_text)\n",
    "    # Remove links to files in the repository which start with / or ./ or ../\n",
    "    readme_text = re.sub(r\"/.*|./.*|../.*\", \"\", readme_text)\n",
    "    # Convert /n to space\n",
    "    readme_text = re.sub(r\"\\n\", \" \", readme_text)\n",
    "    # Make all text lowercase\n",
    "    readme_text = readme_text.lower()\n",
    "    # Only keep Alphanumeric characters and - and _\n",
    "    readme_text = re.sub(r\"[^a-z0-9-_ ]\", \"\", readme_text)\n",
    "    # Remove multiple spaces\n",
    "    readme_text = re.sub(r\" +\", \" \", readme_text)\n",
    "    # Remove empty strings\n",
    "    readme_text = [line for line in readme_text.split(\" \") if line != \"\"]\n",
    "\n",
    "    return readme_text\n",
    "\n",
    "\n",
    "def get_requirements_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the requirements.txt file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the requirements.txt file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "\n",
    "    txt_bool = True\n",
    "    pyproject_bool = False\n",
    "\n",
    "    try:\n",
    "        response = requests.get(f\"{github_link}/main/requirements-dev.txt\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/master/requirements-dev.txt\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/main/dev-requirements.txt\")\n",
    "                if response.status_code != 200:\n",
    "                    response = requests.get(f\"{github_link}/master/dev-requirements.txt\")\n",
    "                    if response.status_code != 200:\n",
    "                        txt_bool = False\n",
    "                        response = requests.get(f\"{github_link}/main/environment.yml\")\n",
    "                        if response.status_code != 200:\n",
    "                            response = requests.get(f\"{github_link}/master/environment.yml\")\n",
    "                            if response.status_code != 200:\n",
    "                                pyproject_bool = True\n",
    "                                response = requests.get(f\"{github_link}/main/pyproject.toml\")\n",
    "                                if response.status_code != 200:\n",
    "                                    response = requests.get(f\"{github_link}/master/pyproject.toml\")\n",
    "                                    if response.status_code != 200:\n",
    "                                        pyproject_bool = False\n",
    "                                        txt_bool = True\n",
    "                                        response = requests.get(f\"{github_link}/main/requirements.txt\")\n",
    "                                        if response.status_code != 200:\n",
    "                                            response = requests.get(f\"{github_link}/master/requirements.txt\")\n",
    "                                            if response.status_code != 200:\n",
    "                                                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None                                \n",
    "\n",
    "    requirements_text = response.text\n",
    "\n",
    "    # Clean the text using regex\n",
    "    cleaning_reg = r\"=.*|>.*|~.*|\\[.*\\]|;.*|<.*|!.*\"\n",
    "\n",
    "    if txt_bool:\n",
    "        # Example: \n",
    "            # versioneer[toml]\n",
    "            # cython~=3.0.5\n",
    "            # meson[ninja]==1.2.1\n",
    "            # meson-python==0.13.1\n",
    "            # pytest>=7.3.2\n",
    "            # pytest-cov\n",
    "            # pytest-xdist>=2.2.0\n",
    "            # pytest-qt>=4.2.0\n",
    "        # We only want the package name and not the version or extras\n",
    "        requirements_text = re.sub(r\"\\[.*\\]\", \"\", requirements_text)\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Clean the text using regex\n",
    "        requirements_text = re.sub(f\"{cleaning_reg}\", \"\", requirements_text)\n",
    "        # lower case\n",
    "        requirements_text = requirements_text.lower()\n",
    "        # Convert to list\n",
    "        requirements_text = requirements_text.split(\"\\n\")\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    elif pyproject_bool:\n",
    "        # Example:\n",
    "            # [project]\n",
    "            # name = \"pydata-sphinx-theme\"\n",
    "            # description = \"Bootstrap-based Sphinx theme from the PyData community\"\n",
    "            # readme = \"README.md\"\n",
    "            # requires-python = \">=3.9\"\n",
    "            # dependencies = [\n",
    "            # \"Babel\",\n",
    "            # \"pygments>=2.7\",\n",
    "            # \"accessible-pygments\",\n",
    "            # \"typing-extensions\"\n",
    "            # ]\n",
    "            # [project.optional-dependencies]\n",
    "            # doc = [\n",
    "            # \"numpydoc\",\n",
    "            # \"linkify-it-py\", # for link shortening\n",
    "            # \"rich\",\n",
    "            # # For examples section\n",
    "            # \"myst-parser\"\n",
    "            # ]\n",
    "\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        dependencies = re.findall(r'dependencies = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        optional_dependencies = re.findall(r'optional-dependencies\\]\\n.*? = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        if len(dependencies) == 0:\n",
    "            return None\n",
    "        if len(optional_dependencies) == 0:\n",
    "            optional_dependencies = [\"\"]\n",
    "        \n",
    "        dependencies = re.findall(r'\".*\"', dependencies[0])\n",
    "        optional_dependencies = re.findall(r'\".*\"', optional_dependencies[0])\n",
    "        requirements_text = dependencies + optional_dependencies\n",
    "        # Remove double quotes\n",
    "        requirements_text = [requirement[1:-1] for requirement in requirements_text]\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # lower case\n",
    "        requirements_text = [requirement.lower() for requirement in requirements_text]\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    else:\n",
    "        # Example:\n",
    "            # name: myenv\n",
    "            # channels:\n",
    "            #   - defaults\n",
    "            # dependencies:\n",
    "            #   - numpy\n",
    "            #   - pandas\n",
    "            #   - pip\n",
    "            #   - pip:\n",
    "            #     - matplotlib\n",
    "        \n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Only get the dependencies which start with '- '\n",
    "        requirements_text = re.findall(r\"- .*\", requirements_text)\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # lower case\n",
    "        requirements_text = [requirement.lower() for requirement in requirements_text]\n",
    "        # Convert to list\n",
    "        requirements_text = [requirement[2:] for requirement in requirements_text]\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "           \n",
    "    return requirements_text\n",
    "\n",
    "\n",
    "def node_creator(data: Tuple[str, str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Function that takes a list of tuples with the package name, the link to the PyPI page and the link to the GitHub page and returns a dictionary with the package name as the key and the value is a dictionary with the package name, the link to the PyPI page, the link to the GitHub page, the text of the README.md file and the text of the requirements.txt file.\n",
    "    \n",
    "    data: list\n",
    "        List of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "        \n",
    "    return: dict\n",
    "        Dictionary with the package name as the key and the value is a dictionary with the package name, the link to the PyPI page, the link to the GitHub page, the text of the README.md file and the text of the requirements.txt file.\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    \n",
    "    package, link, github_link = data\n",
    "\n",
    "    readme_text = get_readme_text(github_link)\n",
    "    requirements_text = get_requirements_text(github_link)\n",
    "    if requirements_text is None:\n",
    "        return None\n",
    "    node[package] = {\"package\": package, \"link\": link, \"github_link\": github_link, \"readme_text\": readme_text, \"requirements_text\": requirements_text}\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Test the function\n",
    "test_data = (\"numpy\", \"https://pypi.org/project/numpy/\", \"github.com/numpy/numpy\")\n",
    "test_node = node_creator(test_data)\n",
    "print(test_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "with open('data/all_links_github.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Run the function with threadpool executor to speed up the process\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    nodes = list(tqdm(executor.map(node_creator, data), total=len(data)))\n",
    "\n",
    "# Save the list to a json file\n",
    "with open(\"data/nodes.json\", \"w\") as f:\n",
    "    json.dump(nodes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS THE FINAL CLEANING OF THE DATA IF NEEDED ####\n",
    "with open('data/nodes.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "for node in data:\n",
    "    # lower the keys\n",
    "    node = {key.lower(): value for key, value in node.items()}\n",
    "    for key, value in node.items():\n",
    "        value[\"package\"] = value[\"package\"].lower()\n",
    "        value[\"requirements_text\"] = [requirement.strip() for requirement in value[\"requirements_text\"]]\n",
    "        value[\"requirements_text\"] = [requirement for requirement in value[\"requirements_text\"] if requirement != \"\"]\n",
    "        value[\"requirements_text\"] = [requirement.lower() for requirement in value[\"requirements_text\"]]\n",
    "        value[\"requirements_text\"] = [re.sub(r\"==.*|>=.*|<=.*|~=.*|!=.*|>.*|<.*\", \"\", requirement) for requirement in value[\"requirements_text\"]]\n",
    "    cleaned_data.append(node)\n",
    "\n",
    "with open(\"data/nodes.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make the edgelist \n",
    "# Load the data\n",
    "with open('data/nodes.json', 'r') as f:\n",
    "    nodes = json.load(f)\n",
    "\n",
    "# Get names of the packages\n",
    "node_names = [list(node.keys())[0] for node in nodes]\n",
    "\n",
    "edge_list = []\n",
    "packages_not_in_network = set()\n",
    "\n",
    "for node in tqdm(nodes):\n",
    "    for package in node:\n",
    "        for requirement in node[package][\"requirements_text\"]:\n",
    "            if requirement not in node_names:\n",
    "                packages_not_in_network.add(requirement)\n",
    "                continue\n",
    "            edge_list.append((requirement, package))\n",
    "\n",
    "print(\"Number of packages not in network:\", len(packages_not_in_network))\n",
    "\n",
    "# Save the edge list to a pickle file and csv for visualization\n",
    "with open(\"data/edge_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(edge_list, f)\n",
    "\n",
    "df = pd.DataFrame(edge_list, columns=[\"source\", \"target\"])\n",
    "df.to_csv(\"data/edge_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a dictionary\n",
    "nodes_mapping = {}\n",
    "\n",
    "for node in nodes:\n",
    "    for key, value in node.items():\n",
    "        nodes_mapping[key] = value\n",
    "\n",
    "# Join the readme text into one string\n",
    "for package in nodes_mapping.keys():\n",
    "    text = nodes_mapping[package][\"readme_text\"]\n",
    "    result = \" \".join(text) if text is not None else \"\"\n",
    "    if result:\n",
    "        nodes_mapping[package][\"readme_text\"] = result\n",
    "\n",
    "# Save the dictionary to a csv file\n",
    "df = pd.DataFrame(nodes_mapping).T\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.to_csv(\"data/nodes_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the edge list\n",
    "with open(\"data/edge_list.pkl\", \"rb\") as f:\n",
    "    edge_list = pickle.load(f)\n",
    "\n",
    "# Take 10% of the edge list\n",
    "edge_list_sample = random.sample(edge_list, math.ceil(len(edge_list)*0.1))\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edge_list_sample)\n",
    "\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "print(\"Number of connected components:\", nx.number_weakly_connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of nodes in each connected component\n",
    "connected_components = nx.weakly_connected_components(G)\n",
    "connected_components = [len(component) for component in connected_components]\n",
    "connected_components = sorted(connected_components, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest connected component\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "print(\"Number of nodes in the largest connected component:\", G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network density\n",
    "density = nx.density(G)\n",
    "print(f'Network density: {density}')\n",
    "\n",
    "# Network connection between nodes\n",
    "is_connected = nx.is_weakly_connected(G)\n",
    "print(f'Is the network connected: {is_connected}')\n",
    "\n",
    "# Number of isolated nodes\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "print(f'Number of isolated nodes: {len(isolated_nodes)}')\n",
    "\n",
    "# Degree\n",
    "degree = dict(G.degree())\n",
    "degree_values = list(degree.values())\n",
    "\n",
    "# Strength\n",
    "strength = dict(G.degree(weight='weight'))\n",
    "strength_values = list(strength.values())\n",
    "\n",
    "# Average\n",
    "avg_degree = np.mean(degree_values)\n",
    "avg_strength = np.mean(strength_values)\n",
    "\n",
    "# Median\n",
    "median_degree = np.median(degree_values)\n",
    "median_strength = np.median(strength_values)\n",
    "\n",
    "# Mode\n",
    "mode_degree = max(set(degree_values), key=degree_values.count)\n",
    "mode_strength = max(set(strength_values), key=strength_values.count)\n",
    "\n",
    "# Minimum\n",
    "min_degree = min(degree_values)\n",
    "min_strength = min(strength_values)\n",
    "\n",
    "# Maximum\n",
    "max_degree = max(degree_values)\n",
    "max_strength = max(strength_values)\n",
    "\n",
    "print(f'Degree Analysis:')\n",
    "print(f'Average: {avg_degree:.2f}')\n",
    "print(f'Median: {median_degree}')\n",
    "print(f'Mode: {mode_degree}')\n",
    "print(f'Minimum: {min_degree}')\n",
    "print(f'Maximum: {max_degree}')\n",
    "print(f'\\nStrength Analysis:')\n",
    "print(f'Average: {avg_strength:.2f}')\n",
    "print(f'Median: {median_strength}')\n",
    "print(f'Mode: {mode_strength}')\n",
    "print(f'Minimum: {min_strength}')\n",
    "print(f'Maximum: {max_strength}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph with netwulf\n",
    "wulf.visualize(G, config={'zoom': 0.6, 'node_gravity': 0.2, 'node_charge': -50,\n",
    "                          'link_distance': 20, 'node_size': 15, 'node_size_variation': 0.55})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random network to compare\n",
    "# Calculate the number of edges in the network\n",
    "L = G.number_of_edges()\n",
    "\n",
    "# Calculate the number of nodes in the network\n",
    "N = G.number_of_nodes()\n",
    "\n",
    "# Calculate the average degree of the network\n",
    "k = 2*L/N\n",
    "print(f\"Average degree of the real network: {k:.2f}\")\n",
    "\n",
    "# Calculate the probability p\n",
    "p = k/(N-1)\n",
    "\n",
    "print(f\"Probability of a link between two nodes: {p:.2e}\")\n",
    "\n",
    "# NOTE: The Erdős-Rény model is a random network model where the probability of a link between two nodes is constant and equal to p.\n",
    "#  Erdős-Rény model\n",
    "def generate_random_network(N, p):\n",
    "    # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add N nodes\n",
    "    G.add_nodes_from(range(N))\n",
    "\n",
    "    # Add edges between nodes with probability p\n",
    "    for i in tqdm(range(N-1)):\n",
    "        for j in range(i+1, N):\n",
    "            if np.random.uniform() < p:\n",
    "                G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "G_rand = generate_random_network(N, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Log of N: {math.log(N)}')\n",
    "print(f\"Average degree of the random network: {2*G_rand.number_of_edges()/G_rand.number_of_nodes():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(bins: list[list[int]], hists: list[list[int]], title: str, labels: list[str]):\n",
    "    for bin, hist, label in zip(bins, hists, labels):\n",
    "        plt.plot(bin[:-1], hist, label=label)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distribution of degree for the random network\n",
    "degree_sequence_rd = [d for _, d in G_rand.degree()]\n",
    "hist_rd, bins_rd = np.histogram(degree_sequence_rd, bins=np.logspace(0, 2.2, 13), density=True)\n",
    "make_plot([bins_rd], [hist_rd], 'Degree distribution of the random network', ['Random network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distribution of degree for the real network\n",
    "degree_sequence = [d for _, d in G.degree()]\n",
    "hist_r, bins_r = np.histogram(degree_sequence, bins=np.logspace(0, 2.2, 13), density=True)\n",
    "make_plot([bins_r], [hist_r], 'Degree distribution of the Pypi packages network', ['Pypi packages network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axvline(k, color='black', linestyle='-', label='Real network average degree')\n",
    "plt.axvline(np.mean(degree_sequence_rd), color='red', linestyle='--', label='Random network average degree')\n",
    "make_plot([bins_rd, bins_r], [hist_rd, hist_r], 'Degree distribution of the real and random network', ['Random network', 'Real network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_assortativity(G):\n",
    "    # k_u and k_v are the degrees of nodes u and v, respectively\n",
    "    k_u = []\n",
    "    k_v = []\n",
    "    for u, v in G.edges():\n",
    "        k_u.append(G.degree(u))\n",
    "        k_v.append(G.degree(v))\n",
    "    \n",
    "    for x, y in G.edges():\n",
    "        k_u.append(G.degree(y))\n",
    "        k_v.append(G.degree(x))\n",
    "    \n",
    "    k_u = np.array(k_u)\n",
    "    k_v = np.array(k_v)\n",
    "    r = ((np.mean(k_u * k_v) - np.mean(k_u) * np.mean(k_v)) /\n",
    "         (np.sqrt(np.mean(k_u**2) - np.mean(k_u)**2) * np.sqrt(np.mean(k_v**2) - np.mean(k_v)**2)))\n",
    "\n",
    "    return r\n",
    "\n",
    "r_degree = degree_assortativity(G.to_undirected())\n",
    "\n",
    "print(f\"Degree assortativity of the real network: {r_degree:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuation model: random network with a pre-defined degree sequence\n",
    "def configuration_model(G: nx.Graph):\n",
    "    G_copy = G.copy()\n",
    "    edges = list(G_copy.edges())\n",
    "    idxs = list(range(len(edges)))\n",
    "    num_swaps = 10 * G_copy.number_of_edges()\n",
    "\n",
    "    for _ in range(num_swaps):\n",
    "        # b. Select two edges\n",
    "        idx1, idx2 = random.sample(idxs, 2)\n",
    "        e1, e2 = edges[idx1], edges[idx2]\n",
    "\n",
    "        # NOTE: Gå ind og læs i bogen. Dette step burde ikke indgå i configuration model http://networksciencebook.com/chapter/4#generating-networks setion 4.8\n",
    "        # # Ensure distinct nodes\n",
    "        # if len(set(e1 + e2)) < 4:\n",
    "        #     continue\n",
    "\n",
    "        # c. Flip the direction of e1 50% of the time\n",
    "        if random.random() < 0.5:\n",
    "            e1 = (e1[1], e1[0])\n",
    "\n",
    "        # Step d: Ensure new edges do not exist\n",
    "        if e1[0] not in G_copy.neighbors(e2[1]) and e2[0] not in G_copy.neighbors(e1[1]):\n",
    "            \n",
    "            # Step e: Remove old edges and add new edges\n",
    "            G_copy.remove_edges_from([e1, e2])\n",
    "            G_copy.add_edges_from([(e1[0], e2[1]), (e2[0], e1[1])])\n",
    "    \n",
    "            edges[idx1] = (e1[0], e2[1])\n",
    "            edges[idx2] = (e2[0], e1[1])\n",
    "\n",
    "    return G_copy\n",
    "\n",
    "G_config = configuration_model(G.to_undirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([G.degree(node) == G_config.degree(node) for node in G.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_undir = G.to_undirected()\n",
    "assortativities = []\n",
    "for _ in tqdm(range(20)):\n",
    "    G_config = configuration_model(G_undir)\n",
    "    r_config = degree_assortativity(G_config)\n",
    "    assortativities.append(r_config)\n",
    "\n",
    "# Plot the distribution of the assortativities\n",
    "plt.hist(assortativities, bins=30, label='Random networks')\n",
    "\n",
    "# Plot the assortativity of the original network\n",
    "plt.axvline(r_degree, color='red', linestyle='--', label='Real network')\n",
    "plt.xlabel('Assortativity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Assortativity of the Configuration model network')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_assortativity_directed(G: nx.DiGraph):\n",
    "    # k_u and k_v are the degrees of nodes u and v, respectively\n",
    "    # We get the k_u_in, k_u_out, k_v_in and k_v_out\n",
    "    k_u_in = []\n",
    "    k_u_out = []\n",
    "    k_v_in = []\n",
    "    k_v_out = []\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        k_u_in.append(G.in_degree(u))\n",
    "        k_v_in.append(G.in_degree(v))\n",
    "        k_u_out.append(G.out_degree(u))\n",
    "        k_v_out.append(G.out_degree(v))\n",
    "\n",
    "    k_u_in = np.array(k_u_in)\n",
    "    k_v_in = np.array(k_v_in)\n",
    "    k_u_out = np.array(k_u_out)\n",
    "    k_v_out = np.array(k_v_out)\n",
    "\n",
    "    r_in_in = ((np.mean(k_u_in * k_v_in) - np.mean(k_u_in) * np.mean(k_v_in)) /\n",
    "            (np.sqrt(np.mean(k_u_in**2) - np.mean(k_u_in)**2) * np.sqrt(np.mean(k_v_in**2) - np.mean(k_v_in)**2)))\n",
    "    \n",
    "    r_out_out = ((np.mean(k_u_out * k_v_out) - np.mean(k_u_out) * np.mean(k_v_out)) /\n",
    "            (np.sqrt(np.mean(k_u_out**2) - np.mean(k_u_out)**2) * np.sqrt(np.mean(k_v_out**2) - np.mean(k_v_out)**2)))\n",
    "    \n",
    "    r_in_out = ((np.mean(k_u_in * k_v_out) - np.mean(k_u_in) * np.mean(k_v_out)) /\n",
    "            (np.sqrt(np.mean(k_u_in**2) - np.mean(k_u_in)**2) * np.sqrt(np.mean(k_v_out**2) - np.mean(k_v_out)**2)))\n",
    "    \n",
    "    r_out_in = ((np.mean(k_u_out * k_v_in) - np.mean(k_u_out) * np.mean(k_v_in)) /\n",
    "            (np.sqrt(np.mean(k_u_out**2) - np.mean(k_u_out)**2) * np.sqrt(np.mean(k_v_in**2) - np.mean(k_v_in)**2)))\n",
    "\n",
    "\n",
    "    return r_in_in, r_out_out, r_in_out, r_out_in\n",
    "\n",
    "r_in_in_real, r_out_out_real, r_in_out_real, r_out_in_real = degree_assortativity_directed(G)\n",
    "\n",
    "print(f\"In-in degree assortativity of the real network: {r_in_in_real:.5f}\")\n",
    "print(f\"Out-out degree assortativity of the real network: {r_out_out_real:.5f}\")\n",
    "print(f\"In-out degree assortativity of the real network: {r_in_out_real:.5f}\")\n",
    "print(f\"Out-in degree assortativity of the real network: {r_out_in_real:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_correlation_directed(G: nx.DiGraph):\n",
    "    # For each node with degree k' we want to get the average of neighbor degree in and neighbor degree out\n",
    "\n",
    "    k_u_in = {}\n",
    "    k_u_out = {}\n",
    "    k_v_in = {}\n",
    "    k_v_out = {}\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        if G.in_degree(u) not in k_u_in:\n",
    "            k_u_in[G.in_degree(u)] = []\n",
    "        if G.out_degree(u) not in k_u_out:\n",
    "            k_u_out[G.out_degree(u)] = []\n",
    "        if G.in_degree(v) not in k_v_in:\n",
    "            k_v_in[G.in_degree(v)] = []\n",
    "        if G.out_degree(v) not in k_v_out:\n",
    "            k_v_out[G.out_degree(v)] = []\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        k_u_in[G.in_degree(u)].append(G.in_degree(v))\n",
    "        k_u_out[G.out_degree(u)].append(G.out_degree(v))\n",
    "        k_v_in[G.in_degree(v)].append(G.in_degree(u))\n",
    "        k_v_out[G.out_degree(v)].append(G.out_degree(u))\n",
    "    \n",
    "    \n",
    "    k_u_in = {k: np.mean(v) for k, v in k_u_in.items()}\n",
    "    k_u_out = {k: np.mean(v) for k, v in k_u_out.items()}\n",
    "    k_v_in = {k: np.mean(v) for k, v in k_v_in.items()}\n",
    "    k_v_out = {k: np.mean(v) for k, v in k_v_out.items()}\n",
    "\n",
    "    return k_u_in, k_u_out, k_v_in, k_v_out\n",
    "\n",
    "    \n",
    "k_u_in, k_u_out, k_v_in, k_v_out = degree_correlation_directed(G)\n",
    "print(\"Average neighbor in-degree of node with in-degree k':\", k_u_in)\n",
    "print(\"Average neighbor out-degree of node with out-degree k':\", k_u_out)\n",
    "print(\"Average neighbor in-degree of node with in-degree k':\", k_v_in)\n",
    "print(\"Average neighbor out-degree of node with out-degree k':\", k_v_out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the x values from the keys of the dictionaries\n",
    "x_values = list(range(0,1000))\n",
    "\n",
    "# Get the y values from the values of the dictionaries\n",
    "y_values_in_in = [k_u_in.get(x, np.nan) for x in x_values]\n",
    "y_values_out_out = [k_u_out.get(x, np.nan) for x in x_values]\n",
    "y_values_in_out = [k_v_in.get(x, np.nan) for x in x_values]\n",
    "y_values_out_in = [k_v_out.get(x, np.nan) for x in x_values]\n",
    "\n",
    "# Plot the degree correlation\n",
    "plt.plot(x_values, y_values_in_in, label='In-in degree correlation')\n",
    "plt.plot(x_values, y_values_out_out, label='Out-out degree correlation')\n",
    "plt.plot(x_values, y_values_in_out, label='In-out degree correlation')\n",
    "plt.plot(x_values, y_values_out_in, label='Out-in degree correlation')\n",
    "\n",
    "# logscale\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Average neighbor degree\")\n",
    "\n",
    "# Place the legend top right corner\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do a degree preserving randomisation : R-S randomization to the plot\n",
    "def degree_preserving_randomization(G: nx.DiGraph):\n",
    "    # Create a copy of the graph\n",
    "    G_copy = G.copy()\n",
    "\n",
    "    # Get the edges\n",
    "    edges = list(G_copy.edges())\n",
    "\n",
    "    # Get the number of edges\n",
    "    num_swaps = 10 * G_copy.number_of_edges()\n",
    "\n",
    "    # For each swap\n",
    "    for _ in range(num_swaps):\n",
    "        # Select two sets of connected\n",
    "        u, v = random.choice(edges)\n",
    "        x, y = random.choice(edges)\n",
    "\n",
    "        # Ensure distinct nodes\n",
    "        if len(set([u, v, x, y])) < 4:\n",
    "            continue\n",
    "\n",
    "        # Ensure that the new edges do not exist\n",
    "        if x not in G_copy.neighbors(v) and y not in G_copy.neighbors(u):\n",
    "            # Remove the old edges\n",
    "            G_copy.remove_edges_from([(u, v), (x, y)])\n",
    "\n",
    "            # Add the new edges\n",
    "            G_copy.add_edges_from([(u, y), (x, v)])\n",
    "\n",
    "            # Update the edges\n",
    "            edges[edges.index((u, v))] = (u, y)\n",
    "            edges[edges.index((x, y))] = (x, v)\n",
    "        \n",
    "\n",
    "    return G_copy\n",
    "\n",
    "G_degree_randomized = degree_preserving_randomization(G)\n",
    "k_u_in_randomized, k_u_out_randomized, k_v_in_randomized, k_v_out_randomized = degree_correlation_directed(G_degree_randomized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(0,1000))\n",
    "\n",
    "y_values_in_in_randomized = [k_u_in_randomized.get(x, np.nan) for x in x_values]\n",
    "y_values_out_out_randomized = [k_u_out_randomized.get(x, np.nan) for x in x_values]\n",
    "y_values_in_out_randomized = [k_v_in_randomized.get(x, np.nan) for x in x_values]\n",
    "y_values_out_in_randomized = [k_v_out_randomized.get(x, np.nan) for x in x_values]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.plot(x_values, y_values_in_in, label='In-in degree correlation', color='blue', marker = 'o')\n",
    "plt.plot(x_values, y_values_out_out, label='Out-out degree correlation', color='orange', marker='o')\n",
    "plt.plot(x_values, y_values_in_out, label='In-out degree correlation', color='green', marker='o')\n",
    "plt.plot(x_values, y_values_out_in, label='Out-in degree correlation', color='red', marker='o')\n",
    "\n",
    "plt.plot(x_values, y_values_in_in_randomized, label='In-in degree correlation randomized', linestyle='None', marker = 'D', color = 'blue', markerfacecolor = 'None')\n",
    "plt.plot(x_values, y_values_out_out_randomized, label='Out-out degree correlation randomized', linestyle='None', marker = 'D', color = 'orange', markerfacecolor = 'None') \n",
    "plt.plot(x_values, y_values_in_out_randomized, label='In-out degree correlation randomized', linestyle='None', marker = 'D', color = 'green', markerfacecolor = 'None')\n",
    "plt.plot(x_values, y_values_out_in_randomized, label='Out-in degree correlation randomized', linestyle='None', marker = 'D', color = 'red', markerfacecolor = 'None')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Average neighbor degree\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same two plots with the degree preserving randomization\n",
    "r_in_in_R_S = []\n",
    "r_out_out_R_S = []\n",
    "r_in_out_R_S = []\n",
    "r_out_in_R_S = []\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    G_R_S = degree_preserving_randomization(G)\n",
    "    r_in_in, r_out_out, r_in_out, r_out_in = degree_assortativity_directed(G_R_S)\n",
    "    r_in_in_R_S.append(r_in_in)\n",
    "    r_out_out_R_S.append(r_out_out)\n",
    "    r_in_out_R_S.append(r_in_out)\n",
    "    r_out_in_R_S.append(r_out_in)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the assortativities\n",
    "plt.hist(r_in_in_R_S, bins=30, label='In-in degree assortativity', color='red')\n",
    "plt.hist(r_out_out_R_S, bins=30, label='Out-out degree assortativity', color='green')\n",
    "plt.hist(r_in_out_R_S, bins=30, label='In-out degree assortativity', color='blue')\n",
    "plt.hist(r_out_in_R_S, bins=30, label='Out-in degree assortativity', color='orange')\n",
    "\n",
    "# Plot the assortativity of the original network\n",
    "plt.axvline(r_in_in_real, color='red', linestyle='--', label='Real network In-in')\n",
    "plt.axvline(r_out_out_real, color='green', linestyle='--', label='Real network Out-out')\n",
    "plt.axvline(r_in_out_real, color='blue', linestyle='--', label='Real network In-out')\n",
    "plt.axvline(r_out_in_real, color='orange', linestyle='--', label='Real network Out-in')\n",
    "\n",
    "plt.xlabel('Assortativity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Assortativity of the random networks (R-S randomization)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead plot it as a line plot where the x_values are in_in, in_out, out_in, out_out:\n",
    "x_values = ['In-in', 'In-out', 'Out-in', 'Out-out']\n",
    "\n",
    "# Get the y values from the values of the dictionaries\n",
    "y_values = [r_in_in_real, r_in_out_real, r_out_in_real, r_out_out_real]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(x_values, y_values, label='Real network')\n",
    "for i, (r_in_in, r_out_out, r_in_out, r_out_in) in enumerate(zip(r_in_in_R_S, r_out_out_R_S, r_in_out_R_S, r_out_in_R_S)):\n",
    "    y_values = [r_in_in, r_in_out, r_out_in, r_out_out]\n",
    "    plt.plot(x_values, y_values, alpha=0.25, color='red')\n",
    "\n",
    "plt.xlabel(\"Degree correlation\")\n",
    "plt.ylabel(\"Assortativity\")\n",
    "plt.title(\"Assortativity of the random networks (R-S randomization)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of Conclusion on the Graph Analysis.\n",
    "The network of github is not random and it shows very clear signs of dissortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 most central packages according to degree centrality.\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'The 5 most central scientists according to closeness centrality are: {[str(package) for package, centrality in sorted_closeness_centrality[:5] ]}')\n",
    "\n",
    "# Find the 5 most central packages according to eigenvector centrality.\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'The 5 most central scientists according to eigenvector centrality are: {[str(package) for package, centrality in sorted_eigenvector_centrality[:5] ]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "degree = dict(G.degree())\n",
    "plt.scatter(list(closeness_centrality.values()), list(degree.values()))\n",
    "plt.xlabel('Closeness centrality')\n",
    "plt.ylabel('Degree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "degree = dict(G.degree())\n",
    "plt.scatter(list(eigenvector_centrality.values()), list(degree.values()))\n",
    "plt.xlabel('Eigenvector centrality')\n",
    "plt.ylabel('Degree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of Conclusion on Graph Analysis.\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual analysis of the found communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and assign all the communities to the dictionary\n",
    "# First make the graph undirected: https://github.com/taynaud/python-louvain/issues/28 \n",
    "# De snakker heller ikke rigtigt om hvordan man skulle gøre i bogen. Derudover er tidskompleksiteten O(L) af louvain\n",
    "\n",
    "G_undir = G.to_undirected()\n",
    "partitioning = community_louvain.best_partition(G_undir)\n",
    "print(len(set(partitioning.values())))\n",
    "print({k: len([v for v in partitioning.values() if v == k]) for k in set(partitioning.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_undir = G.to_undirected()\n",
    "partition = nx.community.louvain_communities(G_undir)\n",
    "print(len(partition))\n",
    "print({i: len([v for v in p]) for i, p in enumerate(partition)})\n",
    "print(nx.community.modularity(G_undir, partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [partitioning[node] for node in G.nodes]\n",
    "nx.draw(G, with_labels=False, node_color=colors)\n",
    "# TODO: Show the modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map a community number into one of 47 unique colors\n",
    "# https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "cmap = plt.get_cmap('gist_ncar')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, )]\n",
    "colors = [colors[community] for community in partitioning.values()]\n",
    "\n",
    "# Map the color to the corresponding package and save the pairs into a csv file\n",
    "community_color = {package: color for package, color in zip(G.nodes, colors), partitioning}\n",
    "community_color = pd.DataFrame(community_color.items(), columns=['id', 'color'])\n",
    "community_color.to_csv(\"data/community_color.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/nodes_data.csv\")\n",
    "df = df.dropna(subset=['readme_text'])\n",
    "# # Take 1/10 of the data\n",
    "# df_subset = df.sample(frac=0.1, random_state=42)\n",
    "# df = df_subset\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text, stopwords=stop_words, stemmer=stemmer):\n",
    "    if text is None or '':\n",
    "        return ''\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the readme's\n",
    "df[\"tokens\"] = df[\"readme_text\"].apply(tokenize)\n",
    "\n",
    "# Combine the tokens from all abstracts into one comprehensive list.\n",
    "all_tokens = df[\"tokens\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams and contingency tables\n",
    "\n",
    "# Function that creates bigrams from a list of tokens\n",
    "bigrams = list(make_bigrams(all_tokens))\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# Function that creates a contingency table for a bigram\n",
    "# Create a BigramCollocationFinder object that also stores tables at the same time\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "\n",
    "# Computes for each unique bigram the chi-squared statistic between the observed and expected contingency tables\n",
    "# Filter bigrams based on chi-squared\n",
    "collocations_chisq = finder.score_ngrams(BigramAssocMeasures().chi_sq)\n",
    "\n",
    "# Compute p-values for the chi-squared statistics\n",
    "# Calculate chi-squared to get p-value\n",
    "collocations_p = []\n",
    "for bigram, chi_squared in tqdm(collocations_chisq):\n",
    "    collocations_p.append((bigram, stats.chi2.sf(chi_squared, 1)))\n",
    "\n",
    "# Find the list of bigrams that appear more than 50 times and have p-value smaller than 0.001\n",
    "# Filter collocations based on p-value\n",
    "collocations = [bigram for bigram, p_value in collocations_p if bigram_counts[bigram] > 50 and p_value < 0.001]\n",
    "\n",
    "# Find the top 20 of them by number of occurrences\n",
    "print(f'Top 20 collocations by number of occurrences: {collocations[:20]}')\n",
    "\n",
    "# Combine collocations into a single token\n",
    "collocation_tokens = ['_'.join(bigram) for bigram in collocations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_collocations(text, collocations=collocation_tokens, stopwords=stop_words, stemmer=stemmer):\n",
    "    if text is None or '':\n",
    "        return ''\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Check all tokens and replace collocations with a single token\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if f'{tokens[i]}_{tokens[i + 1]}' in collocations:\n",
    "            tokens[i] = f'{tokens[i]}_{tokens[i + 1]}'\n",
    "            tokens[i + 1] = ''\n",
    "    tokens = [word for word in tokens if word != '']\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the readme's\n",
    "df[\"tokens\"] = df[\"readme_text\"].apply(tokenize_with_collocations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the partitioning for the packages get a coloumn with their partitioning in the df:\n",
    "df['partition'] = df['package'].map(partitioning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 10 most occuring communities by filtering the dataframe on partitioning to get the top 10 most occuring:\n",
    "top_10_communities = df['partition'].value_counts().head(10).index\n",
    "\n",
    "# Filter the dataframe on the top 10 communities\n",
    "df_top_10 = df[df['partition'].isin(top_10_communities)]\n",
    "\n",
    "# Create a dictionary with the top 10 communities and the corresponding tokens\n",
    "top_10_community_terms = {}\n",
    "\n",
    "for community in top_10_communities:\n",
    "    tokens = df_top_10[df_top_10['partition'] == community]['tokens'].sum()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([' '.join(tokens)])\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_10_community_terms[community] = terms\n",
    "\n",
    "# Get the 3 packages with most out nodes in each community\n",
    "top_3_packages = {}\n",
    "\n",
    "for community in top_10_communities:\n",
    "    out_degrees = dict(G.out_degree())\n",
    "    community_nodes = [node for node in G.nodes if partitioning[node] == community]\n",
    "    out_degrees_community = {node: out_degrees[node] for node in community_nodes}\n",
    "    top_3_packages[community] = sorted(out_degrees_community, key=out_degrees_community.get, reverse=True)[:3]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud with the information on the communities:\n",
    "for community in top_10_communities:\n",
    "    terms = top_10_community_terms[community]\n",
    "    text = ' '.join(terms)\n",
    "    wordcloud = wc.WordCloud(width=800, height=400).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.text(700, -30, f'Community {community}', fontsize=14, color='black', ha='center')\n",
    "    \n",
    "    plt.text(0, -50, f'Top 3 packages: {top_3_packages[community][0]}', fontsize=12, color='black', zorder=1)\n",
    "    plt.text(0, -30, f'{top_3_packages[community][1]}', fontsize=12, color='black', zorder=2)\n",
    "    plt.text(0, -10, f'{top_3_packages[community][2]}', fontsize=12, color='black', zorder=3)\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
