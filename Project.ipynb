{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xmlrpc.client as xc\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set client to the PyPI XML-RPC server\n",
    "client = xc.ServerProxy('http://pypi.python.org/pypi')\n",
    "\n",
    "# Get a list of all the packages\n",
    "pypi_packages = client.list_packages()\n",
    "\n",
    "# Save the list of packages\n",
    "with open(\"data/packages.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pypi_packages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_github_link(packages: list) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a list of python packages and returns a list of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \n",
    "    list_of_packages: list\n",
    "        List of python packages to search for.\n",
    "        \n",
    "    return: list\n",
    "        List of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_links = []\n",
    "    for i, package in enumerate(packages):\n",
    "        # The link to the python package\n",
    "        LINK = f\"https://pypi.org/project/{package}/\"\n",
    "        \n",
    "        # Get the HTML content of the page\n",
    "        r = requests.get(LINK)\n",
    "        \n",
    "        # If the request was not successful, alert the user\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Request failed for {package, i}: {r.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        \n",
    "        # Get sidebar with links\n",
    "        sidebar = soup.find(\"div\", {\"class\": \"vertical-tabs__tabs\"})\n",
    "        \n",
    "        # Get all the links in the sidebar\n",
    "        references = [link.get(\"href\") for link in sidebar.find_all(\"a\")]\n",
    "        \n",
    "        # Join into one string to regex in\n",
    "        reference_text = \" \".join([reference for reference in references if reference is not None])\n",
    "        \n",
    "        # Find the first link that contains the word \"github.com\"\n",
    "        github_links = []\n",
    "        for link in re.finditer(r\"github.com(/\\w*|/\\W|[-]\\w*|[-]\\W*)*\", reference_text):\n",
    "            if link.group() != \"github.com/\" and link.group() != \"github.com\":\n",
    "                github_links.append(link.group())\n",
    "        \n",
    "        # If there are no links, append None\n",
    "        if len(github_links) == 0:\n",
    "            github_link = None\n",
    "        \n",
    "        # If there's several take the shortest and alert the user\n",
    "        elif len(github_links) > 1:\n",
    "            print(f\"Several GitHub links found for {package, i}: {github_links}\")\n",
    "            github_link = min(github_links, key=len)\n",
    "        \n",
    "        # If there is just one link, take that out of the list\n",
    "        elif len(github_links) == 1:\n",
    "            github_link = github_links[0]\n",
    "        \n",
    "        # Else alert the user no githublink is found\n",
    "        else:\n",
    "            print(f\"No GitHub link found for {package, i}\")\n",
    "            github_link = None\n",
    "        \n",
    "        # Append the triplet to the list\n",
    "        all_links.append((package, LINK, github_link))\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "\n",
    "# Run the function with threadpool executor to speed up the process - still takes a loooong time so be aware\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    all_links = list(tqdm(executor.map(get_github_link, [pypi_packages]), total=len(pypi_packages)))\n",
    "\n",
    "# Save the list to a json file\n",
    "with open(\"data/all_links_github.json\", \"w\") as f:\n",
    "    json.dump(all_links, f)\n",
    "\n",
    "# Clean the list of None links\n",
    "all_links_c = [(p, l, g) for p, l, g in all_links if g is not None]\n",
    "\n",
    "with open(\"data/all_links_github_c.json\", \"w\") as f:\n",
    "    json.dump(all_links_c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "    \n",
    "print(\"Number of packages on pypi:\", len(pypi_packages))\n",
    "\n",
    "with open('data/all_links_github.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of packages to successfully access the webpage:\", len(data))\n",
    "\n",
    "with open('data/all_links_github_c.json', 'r') as f:\n",
    "    data_clean = json.load(f)\n",
    "print(\"Number of packages to successfully get the github link from:\", len(data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'pydata', 'sphinx', 'theme', 'license', 'badge', 'pypi', 'version', 'badge', 'conda-forge', 'version', 'badge', 'github', 'workflow', 'test', 'status', 'badge', 'read', 'the', 'docs', 'build', 'status', 'badge', 'codecov', 'test', 'coverage', 'percentage', 'badge', 'a', 'clean', 'three-column', 'bootstrap-based', 'sphinx', 'theme', 'by', 'and', 'for', 'the', 'pydata', 'community', '-', 'books', 'documentation', '-', 'bulb', 'examples', '-', 'raised_hands', 'contribute', '-', 'globe_with_meridians', 'translate', 'pydata', 'theme', '-', 'configure', 'the', 'search', 'position', 'demo', 'image', 'showcasing', 'both', 'the', 'light', 'and', 'dark', 'theme', 'in', 'a', 'single', 'image', 'installation', 'and', 'usage', 'the', 'theme', 'is', 'available', 'on', 'pypi', 'and', 'conda-forge', 'you', 'can', 'install', 'and', 'use', 'as', 'follows', '-', 'install', 'the', 'pydata-sphinx-theme', 'in', 'your', 'doc', 'build', 'environment', 'bash', 'pip', 'install', 'pydata-sphinx-theme', 'or', 'conda', 'install', 'pydata-sphinx-theme', '--channel', 'conda-forge', '-', 'then', 'in', 'the', 'confpy', 'of', 'your', 'sphinx', 'docs', 'you', 'update', 'the', 'html_theme', 'configuration', 'option', 'python', 'html_theme', 'pydata_sphinx_theme', 'and', 'thats', 'it', 'note', 'this', 'theme', 'may', 'not', 'work', 'with', 'the', 'latest', 'major', 'versions', 'of', 'sphinx', 'especially', 'if', 'they', 'have', 'only', 'recently', 'been', 'released', 'please', 'give', 'us', 'a', 'few', 'months', 'of', 'time', 'to', 'work', 'out', 'any', 'bugs', 'and', 'changes', 'when', 'new', 'releases', 'are', 'made', 'see', 'our', 'contributing', 'documentation', 'contribute', 'to', 'and', 'develop', 'the', 'theme', 'contributions', 'are', 'very', 'welcome', 'installing', 'the', 'development', 'version', 'building', 'the', 'example', 'docs', 'and', 'developing', 'the', 'c', 'more', 'detail', 'in', 'the', 'contributing', 'section', 'of', 'the', 'documentation', '-', 'community', 'and', 'contributing', 'documentation']\n"
     ]
    }
   ],
   "source": [
    "# For each package go to the GitHub page and get the readme.text if theres a README.md\n",
    "def get_readme_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the README.md file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the README.md file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"https://github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "    response = requests.get(f\"{github_link}/main/README.md\")\n",
    "    if response.status_code != 200:\n",
    "        response = requests.get(f\"{github_link}/master/README.md\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/main/REAMDE.rst\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/master/REAMDE.rst\")\n",
    "                if response.status_code != 200:\n",
    "                    response = requests.get(f\"{github_link}/main/README.txt\")\n",
    "                    if response.status_code != 200:\n",
    "                        response = requests.get(f\"{github_link}/master/README.txt\")\n",
    "                        if response.status_code != 200:\n",
    "                            return None\n",
    "    \n",
    "    readme_text = response.text\n",
    "\n",
    "    # Remove links which start with http\n",
    "    readme_text = re.sub(r\"http.*\", \"\", readme_text)\n",
    "    # Remove links to files in the repository which start with / or ./ or ../\n",
    "    readme_text = re.sub(r\"/.*|./.*|../.*\", \"\", readme_text)\n",
    "    # Convert /n to space\n",
    "    readme_text = re.sub(r\"\\n\", \" \", readme_text)\n",
    "    # Make all text lowercase\n",
    "    readme_text = readme_text.lower()\n",
    "    # Only keep Alphanumeric characters and - and _\n",
    "    readme_text = re.sub(r\"[^a-z0-9-_ ]\", \"\", readme_text)\n",
    "    # Remove multiple spaces\n",
    "    readme_text = re.sub(r\" +\", \" \", readme_text)\n",
    "    # Remove empty strings\n",
    "    readme_text = [line for line in readme_text.split(\" \") if line != \"\"]\n",
    "\n",
    "\n",
    "    return readme_text\n",
    "\n",
    "\n",
    "def get_requirements_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the requirements.txt file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the requirements.txt file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"https://github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "    response = requests.get(f\"{github_link}/main/requirements-dev.txt\")\n",
    "\n",
    "    txt_bool = True\n",
    "    pyproject_bool = False\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        response = requests.get(f\"{github_link}/master/requirements-dev.txt\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/main/dev-requirements.txt\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/master/dev-requirements.txt\")\n",
    "                if response.status_code != 200:\n",
    "                    txt_bool = False\n",
    "                    response = requests.get(f\"{github_link}/main/environment.yml\")\n",
    "                    if response.status_code != 200:\n",
    "                        response = requests.get(f\"{github_link}/master/environment.yml\")\n",
    "                        if response.status_code != 200:\n",
    "                            pyproject_bool = True\n",
    "                            response = requests.get(f\"{github_link}/main/pyproject.toml\")\n",
    "                            if response.status_code != 200:\n",
    "                                response = requests.get(f\"{github_link}/master/pyproject.toml\")\n",
    "                                if response.status_code != 200:\n",
    "                                    pyproject_bool = False\n",
    "                                    txt_bool = True\n",
    "                                    response = requests.get(f\"{github_link}/main/requirements.txt\")\n",
    "                                    if response.status_code != 200:\n",
    "                                        response = requests.get(f\"{github_link}/master/requirements.txt\")\n",
    "                                        if response.status_code != 200:\n",
    "                                            response = requests.get(f\"{github_link}/main/requirements.txt\")\n",
    "                                \n",
    "\n",
    "    requirements_text = response.text\n",
    "\n",
    "    # Clean the text using regex\n",
    "    cleaning_reg = r\"=.*|>.*|~.*|\\[.*\\]|;.*|<.*|!.*\"\n",
    "\n",
    "    if txt_bool:\n",
    "        # Example: \n",
    "            # versioneer[toml]\n",
    "            # cython~=3.0.5\n",
    "            # meson[ninja]==1.2.1\n",
    "            # meson-python==0.13.1\n",
    "            # pytest>=7.3.2\n",
    "            # pytest-cov\n",
    "            # pytest-xdist>=2.2.0\n",
    "            # pytest-qt>=4.2.0\n",
    "        # We only want the package name and not the version or extras\n",
    "        requirements_text = re.sub(r\"\\[.*\\]\", \"\", requirements_text)\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Convert to list\n",
    "        requirements_text = requirements_text.split(\"\\n\")\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    elif pyproject_bool:\n",
    "        # Example:\n",
    "            # [project]\n",
    "            # name = \"pydata-sphinx-theme\"\n",
    "            # description = \"Bootstrap-based Sphinx theme from the PyData community\"\n",
    "            # readme = \"README.md\"\n",
    "            # requires-python = \">=3.9\"\n",
    "            # dependencies = [\n",
    "            # \"Babel\",\n",
    "            # \"pygments>=2.7\",\n",
    "            # \"accessible-pygments\",\n",
    "            # \"typing-extensions\"\n",
    "            # ]\n",
    "            # [project.optional-dependencies]\n",
    "            # doc = [\n",
    "            # \"numpydoc\",\n",
    "            # \"linkify-it-py\", # for link shortening\n",
    "            # \"rich\",\n",
    "            # # For examples section\n",
    "            # \"myst-parser\"\n",
    "            # ]\n",
    "\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        dependencies = re.findall(r'dependencies = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        optional_dependencies = re.findall(r'optional-dependencies\\]\\n.*? = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        dependencies = re.findall(r'\".*\"', dependencies[0])\n",
    "        optional_dependencies = re.findall(r'\".*\"', optional_dependencies[0])\n",
    "        requirements_text = dependencies + optional_dependencies\n",
    "        # Remove double quotes\n",
    "        requirements_text = [requirement[1:-1] for requirement in requirements_text]\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    else:\n",
    "        # Example:\n",
    "            # name: myenv\n",
    "            # channels:\n",
    "            #   - defaults\n",
    "            # dependencies:\n",
    "            #   - numpy\n",
    "            #   - pandas\n",
    "            #   - pip\n",
    "            #   - pip:\n",
    "            #     - matplotlib\n",
    "        \n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Only get the dependencies which start with '- '\n",
    "        requirements_text = re.findall(r\"- .*\", requirements_text)\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # Convert to list\n",
    "        requirements_text = [requirement[2:] for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "           \n",
    "\n",
    "    return requirements_text\n",
    "\n",
    "text = get_readme_text(\"https://github.com/pydata/pydata-sphinx-theme\")\n",
    "req_text = get_requirements_text(\"https://github.com/pydata/pydata-sphinx-theme\")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
