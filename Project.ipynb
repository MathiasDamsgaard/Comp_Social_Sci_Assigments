{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Union, Optional, Callable\n",
    "# Modules\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xmlrpc.client as xc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import netwulf as wulf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import community as community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set client to the PyPI XML-RPC server\n",
    "client = xc.ServerProxy('http://pypi.python.org/pypi')\n",
    "\n",
    "# Get a list of all the packages\n",
    "pypi_packages = client.list_packages()\n",
    "\n",
    "# lowercase all the package names\n",
    "pypi_packages = [package.lower() for package in pypi_packages]\n",
    "\n",
    "# Save the list of packages\n",
    "with open(\"data/packages.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pypi_packages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_github_link(packages: list) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a list of python packages and returns a list of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \n",
    "    list_of_packages: list\n",
    "        List of python packages to search for.\n",
    "        \n",
    "    return: list\n",
    "        List of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_links = []\n",
    "    for i, package in enumerate(packages):\n",
    "        # The link to the python package\n",
    "        LINK = f\"https://pypi.org/project/{package}/\"\n",
    "        \n",
    "        # Get the HTML content of the page\n",
    "        r = requests.get(LINK)\n",
    "        \n",
    "        # If the request was not successful, alert the user\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Request failed for {package, i}: {r.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        \n",
    "        # Get sidebar with links\n",
    "        sidebar = soup.find(\"div\", {\"class\": \"vertical-tabs__tabs\"})\n",
    "        \n",
    "        # Get all the links in the sidebar\n",
    "        references = [link.get(\"href\") for link in sidebar.find_all(\"a\")]\n",
    "        \n",
    "        # Join into one string to regex in\n",
    "        reference_text = \" \".join([reference for reference in references if reference is not None])\n",
    "        \n",
    "        # Find the first link that contains the word \"github.com\"\n",
    "        github_links = []\n",
    "        for link in re.finditer(r\"github\\.com(/\\w*|/\\W|[-]\\w*|[-]\\W*)*\", reference_text):\n",
    "            if link.group() != \"github.com/\" and link.group() != \"github.com\":\n",
    "                github_links.append(link.group())\n",
    "        \n",
    "        # If there are no links, append None\n",
    "        if len(github_links) == 0:\n",
    "            github_link = None\n",
    "        \n",
    "        # If there's several take the shortest and alert the user\n",
    "        elif len(github_links) > 1:\n",
    "            print(f\"Several GitHub links found for {package, i}: {github_links}\")\n",
    "            github_link = min(github_links, key=len)\n",
    "        \n",
    "        # If there is just one link, take that out of the list\n",
    "        elif len(github_links) == 1:\n",
    "            github_link = github_links[0]\n",
    "        \n",
    "        # Else alert the user no githublink is found\n",
    "        else:\n",
    "            print(f\"No GitHub link found for {package, i}\")\n",
    "            github_link = None\n",
    "        \n",
    "        # Append the triplet to the list\n",
    "        all_links.append((package, LINK, github_link))\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "\n",
    "# Run the function with threadpool executor to speed up the process - still takes a loooong time so be aware\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    all_links = list(tqdm(executor.map(get_github_link, [pypi_packages]), total=len(pypi_packages)))\n",
    "\n",
    "# Save the list to a json file\n",
    "with open(\"data/all_links_github.json\", \"w\") as f:\n",
    "    json.dump(all_links, f)\n",
    "\n",
    "# Clean the list of None links\n",
    "all_links_c = [(p, l, g) for p, l, g in all_links if g is not None]\n",
    "\n",
    "with open(\"data/all_links_github_c.json\", \"w\") as f:\n",
    "    json.dump(all_links_c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/packages.pkl\", \"rb\") as f:\n",
    "    pypi_packages = pickle.load(f)\n",
    "print(\"Number of packages on pypi:\", len(pypi_packages))\n",
    "\n",
    "with open('data/all_links_github.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of packages to successfully access the webpage:\", len(data))\n",
    "\n",
    "with open('data/all_links_github_c.json', 'r') as f:\n",
    "    data_clean = json.load(f)\n",
    "print(\"Number of packages to successfully get the github link from:\", len(data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each package go to the GitHub page and get the readme.text if theres a README.md\n",
    "def get_readme_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the README.md file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the README.md file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{github_link}/main/README.md\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/master/README.md\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/main/REAMDE.rst\")\n",
    "                if response.status_code != 200:\n",
    "                    response = requests.get(f\"{github_link}/master/REAMDE.rst\")\n",
    "                    if response.status_code != 200:\n",
    "                        response = requests.get(f\"{github_link}/main/README.txt\")\n",
    "                        if response.status_code != 200:\n",
    "                            response = requests.get(f\"{github_link}/master/README.txt\")\n",
    "                            if response.status_code != 200:\n",
    "                                return None\n",
    "                            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    readme_text = response.text\n",
    "\n",
    "    # Remove links which start with http\n",
    "    readme_text = re.sub(r\"http.*\", \"\", readme_text)\n",
    "    # Remove links to files in the repository which start with / or ./ or ../\n",
    "    readme_text = re.sub(r\"/.*|./.*|../.*\", \"\", readme_text)\n",
    "    # Convert /n to space\n",
    "    readme_text = re.sub(r\"\\n\", \" \", readme_text)\n",
    "    # Make all text lowercase\n",
    "    readme_text = readme_text.lower()\n",
    "    # Only keep Alphanumeric characters and - and _\n",
    "    readme_text = re.sub(r\"[^a-z0-9-_ ]\", \"\", readme_text)\n",
    "    # Remove multiple spaces\n",
    "    readme_text = re.sub(r\" +\", \" \", readme_text)\n",
    "    # Remove empty strings\n",
    "    readme_text = [line for line in readme_text.split(\" \") if line != \"\"]\n",
    "\n",
    "    return readme_text\n",
    "\n",
    "\n",
    "def get_requirements_text(github_link: str) -> list:\n",
    "    \"\"\"\n",
    "    Function that takes a GitHub link and returns the text of the requirements.txt file.\n",
    "    \n",
    "    github_link: str\n",
    "        Link to the GitHub page.\n",
    "        \n",
    "    return: str\n",
    "        Text of the requirements.txt file.\n",
    "    \"\"\"\n",
    "    # If there's no link, return None\n",
    "    if github_link is None:\n",
    "        return None\n",
    "    \n",
    "    github_link = github_link.replace(\"github.com\", \"https://raw.githubusercontent.com\")\n",
    "    \n",
    "\n",
    "    txt_bool = True\n",
    "    pyproject_bool = False\n",
    "\n",
    "    try:\n",
    "        response = requests.get(f\"{github_link}/main/requirements-dev.txt\")\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(f\"{github_link}/master/requirements-dev.txt\")\n",
    "            if response.status_code != 200:\n",
    "                response = requests.get(f\"{github_link}/main/dev-requirements.txt\")\n",
    "                if response.status_code != 200:\n",
    "                    response = requests.get(f\"{github_link}/master/dev-requirements.txt\")\n",
    "                    if response.status_code != 200:\n",
    "                        txt_bool = False\n",
    "                        response = requests.get(f\"{github_link}/main/environment.yml\")\n",
    "                        if response.status_code != 200:\n",
    "                            response = requests.get(f\"{github_link}/master/environment.yml\")\n",
    "                            if response.status_code != 200:\n",
    "                                pyproject_bool = True\n",
    "                                response = requests.get(f\"{github_link}/main/pyproject.toml\")\n",
    "                                if response.status_code != 200:\n",
    "                                    response = requests.get(f\"{github_link}/master/pyproject.toml\")\n",
    "                                    if response.status_code != 200:\n",
    "                                        pyproject_bool = False\n",
    "                                        txt_bool = True\n",
    "                                        response = requests.get(f\"{github_link}/main/requirements.txt\")\n",
    "                                        if response.status_code != 200:\n",
    "                                            response = requests.get(f\"{github_link}/master/requirements.txt\")\n",
    "                                            if response.status_code != 200:\n",
    "                                                response = requests.get(f\"{github_link}/main/requirements.txt\")\n",
    "                                                if response.status_code != 200:\n",
    "                                                    return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None                                \n",
    "\n",
    "    requirements_text = response.text\n",
    "\n",
    "    # Clean the text using regex\n",
    "    cleaning_reg = r\"=.*|>.*|~.*|\\[.*\\]|;.*|<.*|!.*\"\n",
    "\n",
    "    if txt_bool:\n",
    "        # Example: \n",
    "            # versioneer[toml]\n",
    "            # cython~=3.0.5\n",
    "            # meson[ninja]==1.2.1\n",
    "            # meson-python==0.13.1\n",
    "            # pytest>=7.3.2\n",
    "            # pytest-cov\n",
    "            # pytest-xdist>=2.2.0\n",
    "            # pytest-qt>=4.2.0\n",
    "        # We only want the package name and not the version or extras\n",
    "        requirements_text = re.sub(r\"\\[.*\\]\", \"\", requirements_text)\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Clean the text using regex\n",
    "        requirements_text = re.sub(f\"{cleaning_reg}\", \"\", requirements_text)\n",
    "        # lower case\n",
    "        requirements_text = requirements_text.lower()\n",
    "        # Convert to list\n",
    "        requirements_text = requirements_text.split(\"\\n\")\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    elif pyproject_bool:\n",
    "        # Example:\n",
    "            # [project]\n",
    "            # name = \"pydata-sphinx-theme\"\n",
    "            # description = \"Bootstrap-based Sphinx theme from the PyData community\"\n",
    "            # readme = \"README.md\"\n",
    "            # requires-python = \">=3.9\"\n",
    "            # dependencies = [\n",
    "            # \"Babel\",\n",
    "            # \"pygments>=2.7\",\n",
    "            # \"accessible-pygments\",\n",
    "            # \"typing-extensions\"\n",
    "            # ]\n",
    "            # [project.optional-dependencies]\n",
    "            # doc = [\n",
    "            # \"numpydoc\",\n",
    "            # \"linkify-it-py\", # for link shortening\n",
    "            # \"rich\",\n",
    "            # # For examples section\n",
    "            # \"myst-parser\"\n",
    "            # ]\n",
    "\n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        dependencies = re.findall(r'dependencies = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        optional_dependencies = re.findall(r'optional-dependencies\\]\\n.*? = \\[\\n(.*?)\\n\\]', requirements_text, re.DOTALL)\n",
    "        if len(dependencies) == 0:\n",
    "            return None\n",
    "        if len(optional_dependencies) == 0:\n",
    "            optional_dependencies = [\"\"]\n",
    "        \n",
    "        dependencies = re.findall(r'\".*\"', dependencies[0])\n",
    "        optional_dependencies = re.findall(r'\".*\"', optional_dependencies[0])\n",
    "        requirements_text = dependencies + optional_dependencies\n",
    "        # Remove double quotes\n",
    "        requirements_text = [requirement[1:-1] for requirement in requirements_text]\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # lower case\n",
    "        requirements_text = [requirement.lower() for requirement in requirements_text]\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "    else:\n",
    "        # Example:\n",
    "            # name: myenv\n",
    "            # channels:\n",
    "            #   - defaults\n",
    "            # dependencies:\n",
    "            #   - numpy\n",
    "            #   - pandas\n",
    "            #   - pip\n",
    "            #   - pip:\n",
    "            #     - matplotlib\n",
    "        \n",
    "        # Remove comments\n",
    "        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n",
    "        # Only get the dependencies which start with '- '\n",
    "        requirements_text = re.findall(r\"- .*\", requirements_text)\n",
    "        # Clean the text using regex\n",
    "        requirements_text = [re.sub(f\"{cleaning_reg}\", \"\", requirement) for requirement in requirements_text]\n",
    "        # lower case\n",
    "        requirements_text = [requirement.lower() for requirement in requirements_text]\n",
    "        # Convert to list\n",
    "        requirements_text = [requirement[2:] for requirement in requirements_text]\n",
    "        # Remove trailing spaces\n",
    "        requirements_text = [requirement.strip() for requirement in requirements_text]\n",
    "        # Remove empty strings\n",
    "        requirements_text = [requirement for requirement in requirements_text if requirement != \"\"]\n",
    "\n",
    "           \n",
    "    return requirements_text\n",
    "\n",
    "\n",
    "def node_creator(data: Tuple[str, str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Function that takes a list of tuples with the package name, the link to the PyPI page and the link to the GitHub page and returns a dictionary with the package name as the key and the value is a dictionary with the package name, the link to the PyPI page, the link to the GitHub page, the text of the README.md file and the text of the requirements.txt file.\n",
    "    \n",
    "    data: list\n",
    "        List of tuples with the package name, the link to the PyPI page and the link to the GitHub page.\n",
    "        \n",
    "    return: dict\n",
    "        Dictionary with the package name as the key and the value is a dictionary with the package name, the link to the PyPI page, the link to the GitHub page, the text of the README.md file and the text of the requirements.txt file.\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    \n",
    "    package, link, github_link = data\n",
    "\n",
    "    readme_text = get_readme_text(github_link)\n",
    "    requirements_text = get_requirements_text(github_link)\n",
    "    if requirements_text is None:\n",
    "        return None\n",
    "    node[package] = {\"package\": package, \"link\": link, \"github_link\": github_link, \"readme_text\": readme_text, \"requirements_text\": requirements_text}\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Test the function\n",
    "test_data = (\"numpy\", \"https://pypi.org/project/numpy/\", \"github.com/numpy/numpy\")\n",
    "test_node = node_creator(test_data)\n",
    "print(test_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "with open('data/all_links_github_c.json', 'r') as f:\n",
    "    data_clean = json.load(f)\n",
    "\n",
    "# Run the function with threadpool executor to speed up the process\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    nodes = list(tqdm(executor.map(node_creator, data_clean[110000:220000]), total=len(data_clean[110000:220000])))\n",
    "\n",
    "# Save the list to a json file\n",
    "with open(\"data/nodes_1.json\", \"w\") as f:\n",
    "    json.dump(nodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS THE FINAL CLEANING OF THE DATA IF NEEDED ####\n",
    "with open('data/nodes_3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "for node in data:\n",
    "    # lower the keys\n",
    "    node = {key.lower(): value for key, value in node.items()}\n",
    "    for key, value in node.items():\n",
    "        value[\"package\"] = value[\"package\"].lower()\n",
    "        value[\"requirements_text\"] = [requirement.strip() for requirement in value[\"requirements_text\"]]\n",
    "        value[\"requirements_text\"] = [requirement for requirement in value[\"requirements_text\"] if requirement != \"\"]\n",
    "        value[\"requirements_text\"] = [requirement.lower() for requirement in value[\"requirements_text\"]]\n",
    "        value[\"requirements_text\"] = [re.sub(r\"==.*|>=.*|<=.*|~=.*|!=.*|>.*|<.*\", \"\", requirement) for requirement in value[\"requirements_text\"]]\n",
    "\n",
    "    cleaned_data.append(node)\n",
    "\n",
    "\n",
    "with open(\"data/nodes_3.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make the edgelist \n",
    "# Load the data\n",
    "with open('data/nodes_1.json', 'r') as f:\n",
    "    nodes1 = json.load(f)\n",
    "with open('data/nodes_2.json', 'r') as f:\n",
    "    nodes2 = json.load(f)\n",
    "with open('data/nodes_3.json', 'r') as f:\n",
    "    nodes3 = json.load(f)\n",
    "nodes = nodes1 + nodes2 + nodes3\n",
    "\n",
    "edge_list = []\n",
    "\n",
    "packages_not_in_pypi = set()\n",
    "\n",
    "for node in tqdm(nodes):\n",
    "    if node is None:\n",
    "        continue\n",
    "    for package in node:\n",
    "        if node[package][\"requirements_text\"] is None:\n",
    "            continue\n",
    "        for requirement in node[package][\"requirements_text\"]:\n",
    "            if requirement not in pypi_packages:\n",
    "                packages_not_in_pypi.add(requirement)\n",
    "                continue\n",
    "            edge_list.append((requirement, package))\n",
    "\n",
    "print(\"Number of packages not in PyPI:\", len(packages_not_in_pypi))\n",
    "\n",
    "# Save the edge list\n",
    "with open(\"data/edge_list_complete.pkl\", \"wb\") as f:\n",
    "    pickle.dump(edge_list, f)\n",
    "\n",
    "edge_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a dictionary\n",
    "nodes_dict = {}\n",
    "for node in nodes:\n",
    "    if node is None:\n",
    "        continue\n",
    "    for key, value in node.items():\n",
    "        nodes_dict[key] = value\n",
    "\n",
    "with open(\"data/nodes_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nodes_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the edge list\n",
    "with open(\"data/edge_list_complete.pkl\", \"rb\") as f:\n",
    "    edge_list = pickle.load(f)\n",
    "\n",
    "# Take 5% of the edge list\n",
    "edge_list_sample = random.sample(edge_list, math.ceil(len(edge_list)*0.01))\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edge_list_sample)\n",
    "\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "print(\"Number of connected components:\", nx.number_weakly_connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest connected component\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "print(\"Number of nodes in the largest connected component:\", G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph with netwulf\n",
    "wulf.visualize(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random network to compare\n",
    "# Calculate the number of edges in the network\n",
    "L = G.number_of_edges()\n",
    "\n",
    "# Calculate the number of nodes in the network\n",
    "N = G.number_of_nodes()\n",
    "\n",
    "# Calculate the average degree of the network\n",
    "k = 2*L/N\n",
    "print(f\"Average degree of the real network: {k:.2f}\")\n",
    "\n",
    "# Calculate the probability p\n",
    "p = k/(N-1)\n",
    "\n",
    "print(f\"Probability of a link between two nodes: {p:.2e}\")\n",
    "\n",
    "# NOTE: The Erdős-Rény model is a random network model where the probability of a link between two nodes is constant and equal to p.\n",
    "#  Erdős-Rény model\n",
    "def generate_random_network(N, p):\n",
    "    # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add N nodes\n",
    "    G.add_nodes_from(range(N))\n",
    "\n",
    "    # Add edges between nodes with probability p\n",
    "    for i in tqdm(range(N-1)):\n",
    "        for j in range(i+1, N):\n",
    "            if np.random.uniform() < p:\n",
    "                G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "G_rand = generate_random_network(N, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Log of N: {math.log(N)}')\n",
    "print(f\"Average degree of the random network: {2*G_rand.number_of_edges()/G_rand.number_of_nodes():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(bins: list[list[int]], hists: list[list[int]], title: str, labels: list[str]):\n",
    "    for bin, hist, label in zip(bins, hists, labels):\n",
    "        plt.plot(bin[:-1], hist, label=label)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distribution of degree for the random network\n",
    "degree_sequence_rd = [d for _, d in G_rand.degree()]\n",
    "hist_rd, bins_rd = np.histogram(degree_sequence_rd, bins=np.logspace(0, 2.2, 13), density=True)\n",
    "make_plot([bins_rd], [hist_rd], 'Degree distribution of the random network', ['Random network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distribution of degree for the real network\n",
    "degree_sequence = [d for _, d in G.degree()]\n",
    "hist_r, bins_r = np.histogram(degree_sequence, bins=np.logspace(0, 2.2, 13), density=True)\n",
    "make_plot([bins_r], [hist_r], 'Degree distribution of the Pypi packages network', ['Pypi packages network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axvline(k, color='black', linestyle='-', label='Real network average degree')\n",
    "plt.axvline(np.mean(degree_sequence_rd), color='red', linestyle='--', label='Random network average degree')\n",
    "make_plot([bins_rd, bins_r], [hist_rd, hist_r], 'Degree distribution of the real and random network', ['Random network', 'Real network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_assortativity(G):\n",
    "    # k_u and k_v are the degrees of nodes u and v, respectively\n",
    "    k_u = []\n",
    "    k_v = []\n",
    "    for u, v in G.edges():\n",
    "        k_u.append(G.degree(u))\n",
    "        k_v.append(G.degree(v))\n",
    "    \n",
    "    for x, y in G.edges():\n",
    "        k_u.append(G.degree(y))\n",
    "        k_v.append(G.degree(x))\n",
    "    \n",
    "    k_u = np.array(k_u)\n",
    "    k_v = np.array(k_v)\n",
    "    r = ((np.mean(k_u * k_v) - np.mean(k_u) * np.mean(k_v)) /\n",
    "         (np.sqrt(np.mean(k_u**2) - np.mean(k_u)**2) * np.sqrt(np.mean(k_v**2) - np.mean(k_v)**2)))\n",
    "\n",
    "    return r\n",
    "\n",
    "r_degree = degree_assortativity(G.to_undirected())\n",
    "\n",
    "print(f\"Degree assortativity of the real network: {r_degree:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuation model: random network with a pre-defined degree sequence\n",
    "def configuration_model(G: nx.Graph):\n",
    "    G_copy = G.copy()\n",
    "    edges = list(G_copy.edges())\n",
    "    idxs = list(range(len(edges)))\n",
    "    num_swaps = 10 * G_copy.number_of_edges()\n",
    "\n",
    "    for _ in range(num_swaps):\n",
    "        # b. Select two edges\n",
    "        idx1, idx2 = random.sample(idxs, 2)\n",
    "        e1, e2 = edges[idx1], edges[idx2]\n",
    "\n",
    "        # NOTE: Gå ind og læs i bogen. Dette step burde ikke indgå i configuration model http://networksciencebook.com/chapter/4#generating-networks setion 4.8\n",
    "        # # Ensure distinct nodes\n",
    "        # if len(set(e1 + e2)) < 4:\n",
    "        #     continue\n",
    "\n",
    "        # c. Flip the direction of e1 50% of the time\n",
    "        if random.random() < 0.5:\n",
    "            e1 = (e1[1], e1[0])\n",
    "\n",
    "        # Step d: Ensure new edges do not exist\n",
    "        if e1[0] not in G_copy.neighbors(e2[1]) and e2[0] not in G_copy.neighbors(e1[1]):\n",
    "            \n",
    "            # Step e: Remove old edges and add new edges\n",
    "            G_copy.remove_edges_from([e1, e2])\n",
    "            G_copy.add_edges_from([(e1[0], e2[1]), (e2[0], e1[1])])\n",
    "    \n",
    "            edges[idx1] = (e1[0], e2[1])\n",
    "            edges[idx2] = (e2[0], e1[1])\n",
    "\n",
    "    return G_copy\n",
    "\n",
    "G_config = configuration_model(G.to_undirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([G.degree(node) == G_config.degree(node) for node in G.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_undir = G.to_undirected()\n",
    "assortativities = []\n",
    "for _ in tqdm(range(20)):\n",
    "    G_config = configuration_model(G_undir)\n",
    "    r_config = degree_assortativity(G_config)\n",
    "    assortativities.append(r_config)\n",
    "\n",
    "# Plot the distribution of the assortativities\n",
    "plt.hist(assortativities, bins=30, label='Random networks')\n",
    "\n",
    "# Plot the assortativity of the original network\n",
    "plt.axvline(r_degree, color='red', linestyle='--', label='Real network')\n",
    "plt.xlabel('Assortativity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Assortativity of the Configuration model network')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_assortativity_directed(G: nx.DiGraph):\n",
    "    # k_u and k_v are the degrees of nodes u and v, respectively\n",
    "    # We get the k_u_in, k_u_out, k_v_in and k_v_out\n",
    "    k_u_in = []\n",
    "    k_u_out = []\n",
    "    k_v_in = []\n",
    "    k_v_out = []\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        k_u_in.append(G.in_degree(u))\n",
    "        k_v_in.append(G.in_degree(v))\n",
    "        k_u_out.append(G.out_degree(u))\n",
    "        k_v_out.append(G.out_degree(v))\n",
    "\n",
    "    k_u_in = np.array(k_u_in)\n",
    "    k_v_in = np.array(k_v_in)\n",
    "    k_u_out = np.array(k_u_out)\n",
    "    k_v_out = np.array(k_v_out)\n",
    "\n",
    "    r_in_in = ((np.mean(k_u_in * k_v_in) - np.mean(k_u_in) * np.mean(k_v_in)) /\n",
    "            (np.sqrt(np.mean(k_u_in**2) - np.mean(k_u_in)**2) * np.sqrt(np.mean(k_v_in**2) - np.mean(k_v_in)**2)))\n",
    "    \n",
    "    r_out_out = ((np.mean(k_u_out * k_v_out) - np.mean(k_u_out) * np.mean(k_v_out)) /\n",
    "            (np.sqrt(np.mean(k_u_out**2) - np.mean(k_u_out)**2) * np.sqrt(np.mean(k_v_out**2) - np.mean(k_v_out)**2)))\n",
    "    \n",
    "    r_in_out = ((np.mean(k_u_in * k_v_out) - np.mean(k_u_in) * np.mean(k_v_out)) /\n",
    "            (np.sqrt(np.mean(k_u_in**2) - np.mean(k_u_in)**2) * np.sqrt(np.mean(k_v_out**2) - np.mean(k_v_out)**2)))\n",
    "    \n",
    "    r_out_in = ((np.mean(k_u_out * k_v_in) - np.mean(k_u_out) * np.mean(k_v_in)) /\n",
    "            (np.sqrt(np.mean(k_u_out**2) - np.mean(k_u_out)**2) * np.sqrt(np.mean(k_v_in**2) - np.mean(k_v_in)**2)))\n",
    "\n",
    "\n",
    "    return r_in_in, r_out_out, r_in_out, r_out_in\n",
    "\n",
    "r_in_in_real, r_out_out_real, r_in_out_real, r_out_in_real = degree_assortativity_directed(G)\n",
    "\n",
    "print(f\"In-in degree assortativity of the real network: {r_in_in_real:.5f}\")\n",
    "print(f\"Out-out degree assortativity of the real network: {r_out_out_real:.5f}\")\n",
    "print(f\"In-out degree assortativity of the real network: {r_in_out_real:.5f}\")\n",
    "print(f\"Out-in degree assortativity of the real network: {r_out_in_real:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_correlation_directed(G: nx.DiGraph):\n",
    "    # For each node with degree k' we want to get the average of neighbor degree in and neighbor degree out\n",
    "\n",
    "    k_u_in = {}\n",
    "    k_u_out = {}\n",
    "    k_v_in = {}\n",
    "    k_v_out = {}\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        if G.in_degree(u) not in k_u_in:\n",
    "            k_u_in[G.in_degree(u)] = []\n",
    "        if G.out_degree(u) not in k_u_out:\n",
    "            k_u_out[G.out_degree(u)] = []\n",
    "        if G.in_degree(v) not in k_v_in:\n",
    "            k_v_in[G.in_degree(v)] = []\n",
    "        if G.out_degree(v) not in k_v_out:\n",
    "            k_v_out[G.out_degree(v)] = []\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        k_u_in[G.in_degree(u)].append(G.in_degree(v))\n",
    "        k_u_out[G.out_degree(u)].append(G.out_degree(v))\n",
    "        k_v_in[G.in_degree(v)].append(G.in_degree(u))\n",
    "        k_v_out[G.out_degree(v)].append(G.out_degree(u))\n",
    "    \n",
    "    \n",
    "    k_u_in = {k: np.mean(v) for k, v in k_u_in.items()}\n",
    "    k_u_out = {k: np.mean(v) for k, v in k_u_out.items()}\n",
    "    k_v_in = {k: np.mean(v) for k, v in k_v_in.items()}\n",
    "    k_v_out = {k: np.mean(v) for k, v in k_v_out.items()}\n",
    "\n",
    "    return k_u_in, k_u_out, k_v_in, k_v_out\n",
    "\n",
    "    \n",
    "k_u_in, k_u_out, k_v_in, k_v_out = degree_correlation_directed(G)\n",
    "print(\"Average neighbor in-degree of node with in-degree k':\", k_u_in)\n",
    "print(\"Average neighbor out-degree of node with out-degree k':\", k_u_out)\n",
    "print(\"Average neighbor in-degree of node with in-degree k':\", k_v_in)\n",
    "print(\"Average neighbor out-degree of node with out-degree k':\", k_v_out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the x values from the keys of the dictionaries\n",
    "x_values = list(range(0,1000))\n",
    "\n",
    "# Get the y values from the values of the dictionaries\n",
    "y_values_in_in = [k_u_in.get(x, np.nan) for x in x_values]\n",
    "y_values_out_out = [k_u_out.get(x, np.nan) for x in x_values]\n",
    "y_values_in_out = [k_v_in.get(x, np.nan) for x in x_values]\n",
    "y_values_out_in = [k_v_out.get(x, np.nan) for x in x_values]\n",
    "\n",
    "# Plot the degree correlation\n",
    "plt.plot(x_values, y_values_in_in, label='In-in degree correlation')\n",
    "plt.plot(x_values, y_values_out_out, label='Out-out degree correlation')\n",
    "plt.plot(x_values, y_values_in_out, label='In-out degree correlation')\n",
    "plt.plot(x_values, y_values_out_in, label='Out-in degree correlation')\n",
    "\n",
    "# logscale\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Average neighbor degree\")\n",
    "\n",
    "# Place the legend top right corner\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do a degree preserving randomisation : R-S randomization to the plot\n",
    "def degree_preserving_randomization(G: nx.DiGraph):\n",
    "    # Create a copy of the graph\n",
    "    G_copy = G.copy()\n",
    "\n",
    "    # Get the edges\n",
    "    edges = list(G_copy.edges())\n",
    "\n",
    "    # Get the number of edges\n",
    "    num_swaps = 10 * G_copy.number_of_edges()\n",
    "\n",
    "    # For each swap\n",
    "    for _ in range(num_swaps):\n",
    "        # Select two sets of connected\n",
    "        u, v = random.choice(edges)\n",
    "        x, y = random.choice(edges)\n",
    "\n",
    "        # Ensure distinct nodes\n",
    "        if len(set([u, v, x, y])) < 4:\n",
    "            continue\n",
    "\n",
    "        # Ensure that the new edges do not exist\n",
    "        if x not in G_copy.neighbors(v) and y not in G_copy.neighbors(u):\n",
    "            # Remove the old edges\n",
    "            G_copy.remove_edges_from([(u, v), (x, y)])\n",
    "\n",
    "            # Add the new edges\n",
    "            G_copy.add_edges_from([(u, y), (x, v)])\n",
    "\n",
    "            # Update the edges\n",
    "            edges[edges.index((u, v))] = (u, y)\n",
    "            edges[edges.index((x, y))] = (x, v)\n",
    "        \n",
    "\n",
    "    return G_copy\n",
    "\n",
    "G_degree_randomized = degree_preserving_randomization(G)\n",
    "k_u_in_randomized, k_u_out_randomized, k_v_in_randomized, k_v_out_randomized = degree_correlation_directed(G_degree_randomized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(0,1000))\n",
    "\n",
    "y_values_in_in_randomized = [k_u_in_randomized.get(x, np.nan) for x in x_values]\n",
    "y_values_out_out_randomized = [k_u_out_randomized.get(x, np.nan) for x in x_values]\n",
    "y_values_in_out_randomized = [k_v_in_randomized.get(x, np.nan) for x in x_values]\n",
    "y_values_out_in_randomized = [k_v_out_randomized.get(x, np.nan) for x in x_values]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.plot(x_values, y_values_in_in, label='In-in degree correlation', color='blue', marker = 'o')\n",
    "plt.plot(x_values, y_values_out_out, label='Out-out degree correlation', color='orange', marker='o')\n",
    "plt.plot(x_values, y_values_in_out, label='In-out degree correlation', color='green', marker='o')\n",
    "plt.plot(x_values, y_values_out_in, label='Out-in degree correlation', color='red', marker='o')\n",
    "\n",
    "plt.plot(x_values, y_values_in_in_randomized, label='In-in degree correlation randomized', linestyle='None', marker = 'D', color = 'blue', markerfacecolor = 'None')\n",
    "plt.plot(x_values, y_values_out_out_randomized, label='Out-out degree correlation randomized', linestyle='None', marker = 'D', color = 'orange', markerfacecolor = 'None') \n",
    "plt.plot(x_values, y_values_in_out_randomized, label='In-out degree correlation randomized', linestyle='None', marker = 'D', color = 'green', markerfacecolor = 'None')\n",
    "plt.plot(x_values, y_values_out_in_randomized, label='Out-in degree correlation randomized', linestyle='None', marker = 'D', color = 'red', markerfacecolor = 'None')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Average neighbor degree\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same two plots with the degree preserving randomization\n",
    "r_in_in_R_S = []\n",
    "r_out_out_R_S = []\n",
    "r_in_out_R_S = []\n",
    "r_out_in_R_S = []\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    G_R_S = degree_preserving_randomization(G)\n",
    "    r_in_in, r_out_out, r_in_out, r_out_in = degree_assortativity_directed(G_R_S)\n",
    "    r_in_in_R_S.append(r_in_in)\n",
    "    r_out_out_R_S.append(r_out_out)\n",
    "    r_in_out_R_S.append(r_in_out)\n",
    "    r_out_in_R_S.append(r_out_in)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the assortativities\n",
    "plt.hist(r_in_in_R_S, bins=30, label='In-in degree assortativity', color='red')\n",
    "plt.hist(r_out_out_R_S, bins=30, label='Out-out degree assortativity', color='green')\n",
    "plt.hist(r_in_out_R_S, bins=30, label='In-out degree assortativity', color='blue')\n",
    "plt.hist(r_out_in_R_S, bins=30, label='Out-in degree assortativity', color='orange')\n",
    "\n",
    "# Plot the assortativity of the original network\n",
    "plt.axvline(r_in_in_real, color='red', linestyle='--', label='Real network In-in')\n",
    "plt.axvline(r_out_out_real, color='green', linestyle='--', label='Real network Out-out')\n",
    "plt.axvline(r_in_out_real, color='blue', linestyle='--', label='Real network In-out')\n",
    "plt.axvline(r_out_in_real, color='orange', linestyle='--', label='Real network Out-in')\n",
    "\n",
    "plt.xlabel('Assortativity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Assortativity of the random networks (R-S randomization)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead plot it as a line plot where the x_values are in_in, in_out, out_in, out_out:\n",
    "x_values = ['In-in', 'In-out', 'Out-in', 'Out-out']\n",
    "\n",
    "# Get the y values from the values of the dictionaries\n",
    "y_values = [r_in_in_real, r_in_out_real, r_out_in_real, r_out_out_real]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(x_values, y_values, label='Real network')\n",
    "for i, (r_in_in, r_out_out, r_in_out, r_out_in) in enumerate(zip(r_in_in_R_S, r_out_out_R_S, r_in_out_R_S, r_out_in_R_S)):\n",
    "    y_values = [r_in_in, r_in_out, r_out_in, r_out_out]\n",
    "    plt.plot(x_values, y_values, alpha=0.25, color='red')\n",
    "\n",
    "plt.xlabel(\"Degree correlation\")\n",
    "plt.ylabel(\"Assortativity\")\n",
    "plt.title(\"Assortativity of the random networks (R-S randomization)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of Conclusion on the Graph Analysis.\n",
    "The network of github is not random and it shows very clear signs of dissortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 most central packages according to degree centrality.\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'The 5 most central scientists according to closeness centrality are: {[str(package) for package, centrality in sorted_closeness_centrality[:5] ]}')\n",
    "\n",
    "# Find the 5 most central packages according to eigenvector centrality.\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'The 5 most central scientists according to eigenvector centrality are: {[str(package) for package, centrality in sorted_eigenvector_centrality[:5] ]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "degree = dict(G.degree())\n",
    "plt.scatter(list(closeness_centrality.values()), list(degree.values()))\n",
    "plt.xlabel('Closeness centrality')\n",
    "plt.ylabel('Degree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "degree = dict(G.degree())\n",
    "plt.scatter(list(eigenvector_centrality.values()), list(degree.values()))\n",
    "plt.xlabel('Eigenvector centrality')\n",
    "plt.ylabel('Degree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of Conclusion on Graph Analysis.\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual analysis of the found communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and assign all the communities to the dictionary\n",
    "# First make the graph undirected: https://github.com/taynaud/python-louvain/issues/28 \n",
    "# De snakker heller ikke rigtigt om hvordan man skulle gøre i bogen. Derudover er tidskompleksiteten O(L) af louvain\n",
    "\n",
    "G_undir = G.to_undirected()\n",
    "\n",
    "partitioning = community_louvain.best_partition(G_undir)\n",
    "print(len(set(partitioning.values())))\n",
    "print({k: len([v for v in partitioning.values() if v == k]) for k in set(partitioning.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [partitioning[node] for node in G.nodes]\n",
    "nx.draw(G, with_labels=False, node_color=colors)\n",
    "# TODO: Show the modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
